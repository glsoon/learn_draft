#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Time    : 2022/10/11 14:35
# @Author  : Gongle
# @File    : Text_Similariy_Sampler_Expl_1011.py
# @Version : 1.0
# @Desc    : None
"""
ä¸‰ç§æ•°æ®å½¢æ€ï¼š
| æ–‡æœ¬ç§ç±» | æ ‡å‡†æ–‡æœ¬ | ç›¸ä¼¼æ–‡æœ¬| æ ‡ç­¾|
|:----:| :----:| :----:| :----:|
|A| A1_Stand_text | A1_Simi_text | label_1 |
|A| A2_Stand_text | A2_Simi_text | label_0 |
|B| B1_Stand_text | B1_Simi_text | label_0 |
|C| C1_Stand_text | C1_Simi_text | label_0 |

ç¬¬ä¸€ç§[éƒ½äº’æ–¥]ï¼šA1,A2,B1,C1éƒ½æ˜¯ç»å¯¹äº’æ–¥ï¼Œä¸åŒç§ç±»
ç¬¬äºŒç§[å¤–äº’æ–¥]ï¼šAå¤§ç±»ä¸­A1,A2ä¸é‚£ä¹ˆäº’æ–¥ï¼Œå¦‚è…¹ç—›ï¼Œä¸‹è…¹ç—›ï¼›é‡ç‚¹ä¸åˆ«çš„å¤§ç±»B,Cä½œåŒºåˆ†ã€‚
ç¬¬ä¸‰ç§[å†…äº’æ–¥]ï¼šAå¤§ç±»ä¸­A1,A2è¦åšç»å¯¹åŒºåˆ†ï¼›ç”±äºä¸åˆ«çš„å¤§ç±»B,Cä¹Ÿæœ‰ç±»ä¼¼çš„æ–‡æœ¬ï¼Œå¦‚Aä¸­è…¹ç—›å¤šä¹…ï¼ŒB,Cæœ‰èƒƒç‚å¤šä¹…ï¼Œå‘çƒ­å¤šä¹…ã€‚
**åŒæ—¶åµŒå…¥æ•°æ®å¢å¼ºæ¨¡æ¨¡å—**
"""

import numpy   as np
import random
import pandas  as pd
import re
import json
import itertools
from datetime import datetime, date
from time import time
from sklearn.model_selection import train_test_split
from sklearn.metrics.pairwise import cosine_similarity
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
import os
import logging
from pprint import pprint

#os.chdir('Disease_10_Doctor_query_v2_1009')

logging.basicConfig(format='%(asctime)s - %(message)s',
                    datefmt='%Y-%m-%d %H:%M:%S',
                    level=logging.INFO)

logger=logging.getLogger(__name__)
logger.info('Starting')


###########################################################################
## Exclusive Strategy
###########################################################################

""" 
è¾“å…¥æœ‰å‡ ç§æ•°æ®ï¼š
1. åªæœ‰ä¸€ä¸ªæ•°æ®,all_data->   æ ¹æ®æ¨¡å¼-'test'      : train_data,dev_data,test_data
                           æˆ–è€…æ¨¡å¼-'train'     : train_data,_,test_dataã€‚
                           æˆ–è€…æ¨¡å¼-'only-train': train_data,_,_ã€‚
2. æƒ³å¿«é€ŸéªŒè¯æ¨¡å‹ä¸Šçº¿ï¼Œtrain_data,test_data
3. å·²ç»æœ‰åˆ‡åˆ†å¥½çš„æ•°æ®ï¼Œtrain_data,dev_data,test_data, éªŒè¯æ¨¡å‹ä»¥åŠè°ƒå‚çš„æœ‰æ•ˆæ€§ã€‚
4. åŸºäºä¸Šé¢çš„æ•°æ®ï¼Œå¦æœ‰soft-è´Ÿæ ·æœ¬æ•°æ®ï¼Œç”¨ä»¥å¢å¼ºæ¨¡å‹å­¦ä¹ ã€‚
5. å¢åŠ æ ·æœ¬å¢å¼ºç­–ç•¥ã€‚
-------------------------------------------
æ•°æ®é›†åŒä¸€ä¸º4åˆ—ï¼š| æ–‡æœ¬ç§ç±»  |  æ ‡å‡†æ–‡æœ¬   |  ç›¸ä¼¼æ–‡æœ¬   | æ ‡ç­¾  |
å¯¹åº”Column-En, |Category | Stand_text | Simi_text | label |
6.labelæœ€å¥½æœ‰æ­£è´Ÿæ ·æœ¬ï¼Œä½†æœ‰æ—¶ä¹Ÿåªæœ‰æ­£æ ·æœ¬ï¼›è¦åˆ†åˆ«è€ƒè™‘å¤„ç†ã€‚
7.æ­£è´Ÿæ ·æœ¬æŠ½æ ·æˆå¯¹é€»è¾‘[åˆå§‹åŒ–]:
    æ­£æ ·æœ¬é€»è¾‘
    7.1.1 æ­£ç›¸ä¼¼é—®ä¹‹é—´æ„é€ æ­£æ ·æœ¬[è¶…è¿‡æ ·æœ¬é‡ï¼Œå¯èƒ½æ— è¯¥æ­£æ ·æœ¬é›†]
    7.1.2 è‡ªæŠ½æ ·å¤åˆ¶æ‰©å……æ­£æ ·æœ¬å¯¹æ•°é‡[3æ¬¡ä¸åŒéšæœºæ“ä½œ]ã€‚
    7.1.3 
    è´Ÿæ ·æœ¬é€»è¾‘ï¼š
    7.2.1 æ ‡å‡†é—®ä¹‹é—´æ„é€ è´Ÿæ ·æœ¬å¯¹,åŒ…å«ä¸å…¶ä»–æ‰€æœ‰é‡å¤ç¬›å¡å°”å’Œå»é‡æ ‡å‡†é—®ç¬›å¡å°”æˆå¯¹ï¼Œæ„æˆè´Ÿæ ·æœ¬å¯¹
    7.2.2 ä¸åŒæ ‡å‡†é—®çš„ç›¸ä¼¼é—®æ„é€ è´Ÿæ ·æœ¬å¯¹[é»˜è®¤é‡å¤10æ¬¡ï¼›2ä¸ªæ ·æœ¬é›†]ã€‚
    7.2.3 é¢å¤–æ‰©å±•æ•°æ®é›†[å‡è®¾ä¸ºæ ‡å‡†é—®]ä¸ç›¸ä¼¼é—®æ„é€ è´Ÿæ ·æœ¬[æ­£è´Ÿæ ·æœ¬å·®é¢é‡]
    7.2.4 é¢å¤–æ‰©å±•æ•°æ®é›†[å‡è®¾ä¸ºç›¸ä¼¼é—®]ä¸å…¶ä»–æ ‡å‡†é—®,æ„é€ è´Ÿæ ·æœ¬å¯¹[åŸå§‹æ•°æ®é‡]
    7.2.5 
8.æ ·æœ¬å¢å¼ºç­–ç•¥

"""

"""
data file:  Doctor_query_all_df_1009.xlsx; 
            Doctor_query_train_df_1009.xlsx; 
            Doctor_query_eval_df_1009.xlsx
"""
#data_file='data/simi_data/Disease_general_norm_6k_220929.xlsx'
data_file='data/simi_data/Med_general_norm_category_1p7w_220929.xlsx'
data_df=pd.read_excel(data_file)

print(data_df.shape)
print(data_df.head())

random_seed=66
random.seed(random_seed)


##########################################################
##  Method-1: åˆå§‹All-Exclusive
##########################################################

def data_split_list(data_list):
    """
    :param data_list: List
    å°†åˆ—è¡¨è¿›è¡Œåˆ†å‰²ï¼Œåˆ†æˆä¸¤ä¸ªäº’æ–¥çš„éƒ¨åˆ†ï¼Œ
    returnï¼šæŠ›å‡ºéšæœºæŸä¸ªå…ƒç´ ï¼Œä¸”ååŠä¸ªéç©ºList
    """
    if len(data_list) == 1:
        data_list_pre = data_list * 4
    elif len(data_list) == 0:
        return None, None
    else:
        data_list_pre = data_list
    n = random.randint(0, len(data_list_pre) - 1)
    return data_list_pre.pop(n), data_list_pre

xx=list(range(6))
x,y=data_split_list(xx)
logger.info(f'x: {x}; y: {y}')


def get_positive_pair(data_list):
    """
    åŒä¸€æ ‡ç­¾æ•°æ®,ä¸­é—´åˆ†å‰²ï¼Œè¿›è¡Œæˆå¯¹
    :param data_list:List
    return:List[(tuple_1,tuple_2),..],å…ƒç´ å¯¹åˆ—è¡¨
    """

    former_num = int(len(data_list) / 2)
    former_latter_pair = zip(data_list[:former_num], data_list[former_num:2 * former_num])
    return list(former_latter_pair)

xx=list(range(5))
x=get_positive_pair(xx)
logger.info(f'x: {x}')

def get_pair_df(stand_simi_dict, force_simi_comb=True):
    """
    ä¸åŒæ ‡å‡†é—®ä¸­ç›¸ä¼¼é—®ä¹‹é—´,æ„æˆè´Ÿæ ·æœ¬å¯¹ï¼›æ­£ç›¸ä¼¼é—®ä¹‹é—´ï¼Œæ„æˆæ­£æ ·æœ¬å¯¹
    :param query_dict: {'Stand_text':'pos_Simi_text'},é’ˆå¯¹[æ ‡å‡†é—®-æ­£ç›¸ä¼¼é—®åˆ—è¡¨]å­—å…¸ï¼Œ
    return: æ ‡å‡†é—®ä¹‹é—´è´Ÿæ ·æœ¬-df,ç›¸ä¼¼é—®ä¹‹é—´æ­£æ ·æœ¬-df,æ–°çš„{'Stand_text':'rest-Simi_text'}å­—å…¸
    """
    ## å­˜å‚¨ä¸åŒæ ‡å‡†é—®ä¸­çš„ç›¸ä¼¼é—®ï¼Œç„¶åå‰ååˆ†å‰²ï¼Œæ„æˆè´Ÿæ ·æœ¬
    stand_simi_text_list = []
    ## ç›¸ä¼¼é—®ä¹‹é—´ï¼Œæ„æˆæ­£æ ·æœ¬
    pos_simi_text_list = []
    ## æ ‡å‡†é—®éšæœºæŠ½å–ç›¸ä¼¼é—®ï¼Œæ„æˆå­—å…¸
    stand_rest_simi_dict = {}
    for sub_stand_text, sub_simi_text_list in stand_simi_dict.items():
        ## sub_stand_text,æ ‡å‡†é—® ;sub_simi_text_list,ç›¸ä¼¼é—®
        rand_simi_text, rest_simi_text_list = data_split_list(sub_simi_text_list)
        if rand_simi_text == None:
            continue
        stand_simi_text_list.append(rand_simi_text)
        if force_simi_comb:
            ## å¼ºåˆ¶ç›¸ä¼¼æ ·æœ¬ç»„åˆæˆå¯¹
            rest_simi_text_pair_list = get_positive_pair(rest_simi_text_list)
            pos_simi_text_list.extend(rest_simi_text_pair_list)
        elif len(rest_simi_text_list) // 2:
            ##éšæœºé€»è¾‘ï¼Œå½“ä¸ºå¥‡æ•°æ—¶ï¼Œç›¸ä¼¼æ ·æœ¬ç»„åˆæˆå¯¹
            rest_simi_text_pair_list = get_positive_pair(rest_simi_text_list)
            pos_simi_text_list.extend(rest_simi_text_pair_list)
        ## ç›¸ä¼¼æ ·æœ¬ä¸ç»„åˆæˆå¯¹
        stand_rest_simi_dict.update({sub_stand_text: rest_simi_text_list})
    ## æ ‡å‡†é—®é¡ºåºï¼Œæ„æˆæˆå¯¹ï¼Œæœ€ç»ˆè´Ÿæ ·æœ¬ï¼Œ
    neg_stand_pair_list = get_positive_pair(stand_simi_text_list)
    return pd.DataFrame(neg_stand_pair_list), pd.DataFrame(pos_simi_text_list), stand_rest_simi_dict


xx={'a':list('bcdef'),'h':list('ijkmn')}
logger.info(f'stand_simi_dict: {xx}')
neg_df,pos_df,stand_rest_simi_dict=get_pair_df(xx)
logger.info(f'processed stand_simi_dict: {xx}')
logger.info(f'neg_df:\n{neg_df}')
logger.info(f'pos_df:\n{pos_df}')
logger.info(f'stand_rest_simi_dict: {stand_rest_simi_dict}')

## empty dict
xx={}
logger.info(f'stand_simi_dict: {xx}')
neg_df,pos_df,stand_rest_simi_dict=get_pair_df(xx)

from itertools import combinations,product

xx=['ab','cd','ef']
x_com=combinations(xx,2)
logger.info('combination: {}'.format(list(x_com)))

x=['a','b']
y=['c','d','e']
xy=list(product(x,y))
logger.info('product x_len: {}, y_len: {}; x_y_len: {}.'.format(len(x),len(y),len(xy)))
logger.info('product examples:: {}'.format(list(xy)))


def sample_focus_neg_df_pre(data_df):
    """
    æ ‡å‡†é—®ä¹‹é—´ï¼Œè´Ÿæ ·æœ¬ï¼›æ­£ç›¸ä¼¼é—®ä¹‹é—´ï¼Œæ­£æ ·æœ¬
    å½¢æˆæœ€ç»ˆdataframe
    :param: DataFrame,åŸå§‹æ•°æ®ã€‚
    return: train-df,åŸºäºä¸åŒæ ‡å‡†é—®çš„ç›¸ä¼¼é—®æ„é€ è´Ÿæ ·æœ¬-df,ç›¸ä¼¼é—®æ­£æ ·æœ¬-df(å¯ä»¥ä¸ºç©º),ç»„åˆæœ€ç»ˆdf
    """
    data = data_df.copy()
    ## æ ‡å‡†é—®å¯¹åº”ç›¸ä¼¼é—®åˆ—è¡¨å­—å…¸
    stand_simi_dict = {}
    for one in data.groupby('Stand_text'):
        standard_query = one[0]
        sim_query = one[1]['Simi_text'].tolist()
        if standard_query in sim_query:
            pass
        else:
            sim_query.append(standard_query)
        stand_simi_dict.update({standard_query: sim_query})
    stand_simi_new_dict = stand_simi_dict.copy()
    neg_stand0simi_pair_df = []
    ## é‡å¤10æ¬¡
    ## è¶…è¿‡ä¸€å®šæ¬¡æ•°ï¼Œåªæœ‰è´Ÿæ ·æœ¬æ•°æ®ï¼›æ­£æ ·æœ¬æ•°æ®ä¸ºç©ºã€‚
    for i in range(10):
        sub_neg_stand0simi_df, sub_pos_simi_df, stand_simi_new_dict = get_pair_df(stand_simi_new_dict,force_simi_comb=False)
        neg_stand0simi_pair_df.append(sub_neg_stand0simi_df)
    sub_neg_stand0simi_df, pos_simi_df, stand_simi_new_dict = get_pair_df(stand_simi_new_dict,force_simi_comb=True)
    neg_stand0simi_pair_df.append(sub_neg_stand0simi_df)
    neg_stand_simi_pair_df = pd.concat(neg_stand0simi_pair_df, axis=0,sort=False)
    logger.info(f'pos_simi_df:\n{pos_simi_df.head()}')
    neg_stand_simi_pair_df['label'] = 0
    pos_simi_df['label'] = 1
    train_df = pd.concat([neg_stand_simi_pair_df, pos_simi_df], axis=0,sort=False).reset_index(drop=True)
    train_df.columns=['Stand_text', 'Simi_text', 'label']
    train_df = train_df.sample(frac=1).reset_index(drop=True)
    return train_df


xx=pd.DataFrame({'Stand_text':['a']*5+['h']*5,'Simi_text':list('bcdef')+list('ijkmn')})
logger.info(f'xx:\n{xx}')

train_df=sample_focus_neg_df_pre(xx)
logger.info(f'train_df:\n{train_df}')



def sample_focus_simi_neg_df(data_df,extra_file):
    """
    é‡ç‚¹åŸºäºç›¸ä¼¼è¯­æ–™æ„é€ è´Ÿæ ·æœ¬ï¼›å¦‚æœæ ·æœ¬æ¯”è¾ƒå¤šï¼Œå¯èƒ½é¡ºä¾¿æœ‰æ­£æ ·æœ¬
    1. åŸºäºåŸºäºä¸åŒæ ‡å‡†é—®çš„ç›¸ä¼¼é—®æ„é€ è´Ÿæ ·æœ¬-dfçš„base-method,ç”Ÿæˆdf
    2. é¢å¤–æ ·æœ¬[å‡è®¾ä¸ºæ ‡å‡†é—®]ä¸ç›¸ä¼¼æ ·æœ¬[åŸå§‹ç›¸ä¼¼]ï¼Œæ„é€ è´Ÿæ ·æœ¬é›†ã€‚
    :param data_df:DataFrame,åŸå§‹æ•°æ®ã€‚
    :param extra_file:é¢å¤–æ‰©å±•æ–‡ä»¶ï¼Œcsvæ ¼å¼
    return:train-df,åŸºäº1ï¼Œ2ä¸¤ç§æ€è·¯æ„é€ è´Ÿæ ·æœ¬è®­ç»ƒé›†ã€‚
    """
    train_df1 = sample_focus_neg_df_pre(data_df)
    train_df2 = sample_focus_neg_df_pre(data_df)
    train_data_pre = pd.concat([train_df1, train_df2], axis=0)
    label_dist = train_data_pre.label.value_counts()
    if label_dist.shape[0]>1:
        diff = label_dist[1] - label_dist[0]
        #æ­£è´Ÿæ ·æœ¬æ•°é‡å·®é¢
        diff = abs(diff)
        #print('diff: ',diff)
        data = data_df.copy()
        simi_text_series = data.loc[:, 'Simi_text'].sample(diff, replace=True)
    else:
        diff=int(label_dist.shape[0]/2)
        simi_text_series = train_data_pre.loc[:, 'Simi_text'].sample(diff)
    if extra_file:
        corpus_extra = pd.read_csv(extra_file, header=None)
        logger.info('Extra  data shape: {}'.format(corpus_extra.shape))
        if diff < corpus_extra.shape[0]:
            extra_neg_text = corpus_extra.sample(diff).loc[:, 0]
        else:
            extra_neg_text = corpus_extra.sample(diff, replace=True).loc[:, 0]
        extra_neg_df = pd.DataFrame({'Simi_text': simi_text_series.tolist(), 'Stand_text': extra_neg_text.tolist(), 'label': 0})
        extra_neg_df = extra_neg_df.loc[:, ['Stand_text', 'Simi_text', 'label']]
        all_data = pd.concat([train_data_pre, extra_neg_df], axis=0)
    else:
        all_data=train_data_pre
    all_data = all_data.sample(frac=1)
    return all_data


xx=pd.DataFrame({'Stand_text':['a']*5+['h']*5,'Simi_text':list('bcdef')+list('ijkmn')})
logger.info(f'xx:\n{xx}')

extra_file=None
# extra_file='data/severe_simi_neg_extra_corpus.csv'
train_df=sample_focus_simi_neg_df(xx,extra_file)
logger.info(f'train_df:\n{train_df}')


def build_sample_df(data_df,extra_file):
    """
    1. æ­£æ ·æœ¬è‡ªæŠ½æ ·ï¼Œæ‰©å……æ­£æ ·æœ¬æ•°é‡,æ„é€ æ­£æ ·æœ¬å¯¹
    2. é¢å¤–æ‰©å±•æ ·æœ¬ä¸å…¶ä»–æ ‡å‡†é—®,æ„é€ è´Ÿæ ·æœ¬å¯¹
    3. å½“å‰æ ‡å‡†é—®ä¸å…¶ä»–æ ‡å‡†é—®ï¼ŒåŒ…å«ä¸å…¶ä»–æ‰€æœ‰é‡å¤ç¬›å¡å°”å’Œå»é‡æ ‡å‡†é—®ç¬›å¡å°”æˆå¯¹ï¼Œæ„æˆè´Ÿæ ·æœ¬å¯¹
    :param data_df: åªæœ‰æ­£æ ·æœ¬çš„åŸå§‹df
    :param extra_file: é¢å¤–æ‰©å±•æ–‡ä»¶ï¼Œcsvæ ¼å¼
    return
    """
    ## 1. æ­£æ ·æœ¬è‡ªæŠ½æ ·ï¼Œæ‰©å……æ­£æ ·æœ¬æ•°é‡
    data = data_df.loc[:,['Stand_text','Simi_text']].copy()
    # è‡ªæŠ½æ ·æ‰©å……æ­£æ ·æœ¬æ•°é‡
    data_pos_sample_df1 = data.sample(frac=0.5)
    data_pos_sample_df2 = data.sample(frac=1.5, replace=True)
    data_pos_sample_df3 = data.loc[:, ['Simi_text', 'Stand_text']].sample(frac=0.5)
    standard_query = data['Stand_text'].unique().tolist()
    # 2. é¢å¤–æ‰©å±•æ ·æœ¬ä¸å…¶ä»–æ ‡å‡†é—®ï¼Œæ„é€ è´Ÿæ ·æœ¬ã€‚
    if extra_file:
        data_extra = pd.read_csv(extra_file, header=None)
        data_extra = data_extra[0].unique().tolist()
        ## æ ‡å‡†é—®ä¸é¢å¤–æ ·æœ¬ç¬›å¡å°”ç§¯è·å¾—è´Ÿæ ·æœ¬å¯¹
        data_neg_extra_df4 = pd.DataFrame(list(itertools.product(standard_query, data_extra)))
        data_neg_extra_df4 = data_neg_extra_df4.sample(len(data))
    else:
        data_neg_extra_df4=pd.DataFrame()
    #3.1 å½“å‰æ ‡å‡†é—®ä¸å…¶ä»–æ ‡å‡†é—®ï¼Œä¸å…¶ä»–æ‰€æœ‰é‡å¤æ ‡å‡†é—®ç¬›å¡å°”
    neg_stand_pair_list = []
    for sub_stand in standard_query:
        data2 = data[data['Stand_text'] != sub_stand]
        other_stand = data2['Stand_text'].tolist()
        neg_stand_pair_list.extend(list(itertools.product([sub_stand], other_stand)))
    data_neg_stand_df5 = pd.DataFrame(neg_stand_pair_list)
    data_neg_stand_df5 = data_neg_stand_df5.sample(len(data))
    #3.2 æ ‡å‡†é—®ä¹‹é—´å»é‡æ ‡å‡†é—®ç¬›å¡å°”æˆå¯¹
    stand_unique_list = data['Stand_text'].unique().tolist()
    data_neg_stand_df6_pre = pd.DataFrame(list(itertools.combinations(stand_unique_list, 2)))
    if data_neg_stand_df6_pre.shape[0] > int(data.shape[0] / 2):
        data_neg_stand_df6 = data_neg_stand_df6_pre.sample(int(len(data) / 2))
    else:
        data_neg_stand_df6 = data_neg_stand_df6_pre.sample(int(len(data) / 2), replace=True)

    sample_df_list = [data_pos_sample_df1, data_pos_sample_df2, data_pos_sample_df3,
         data_neg_extra_df4, data_neg_stand_df5, data_neg_stand_df6]
    ## å‰ä¸‰ä¸ªä¸ºæ­£æ ·æœ¬é›†ï¼Œåä¸‰ä¸ªä¸ºè´Ÿæ ·æœ¬é›†
    ##åŒ…å«labelå­—æ®µ
    train_norm_df_list = []
    for idx, sub_sample_df in enumerate(sample_df_list):
        logger.info('file : {}  shape: {}'.format(idx, sub_sample_df.shape))
        if sub_sample_df.shape[0]==0:
            continue

        sub_sample_df.columns = ['Simi_text', 'Stand_text']
        if idx < 3:
            sub_sample_df['label'] = 1
        else:
            sub_sample_df['label'] = 0
        train_norm_df_list.append(sub_sample_df)
    #é‡ç‚¹åŸºäºç›¸ä¼¼é—®æ„é€ è´Ÿæ ·æœ¬å’Œä¸€å®šé‡çš„æ­£æ ·æœ¬
    train_focus_neg_df = sample_focus_simi_neg_df(data_df,extra_file)
    train_focus_neg_df = train_focus_neg_df.sample(len(data))
    train_norm_df_list.append(train_focus_neg_df)
    final_train_df = pd.concat(train_norm_df_list, axis=0, sort=False,ignore_index=True)
    final_train_df = final_train_df.sample(frac=1)
    final_train_df = final_train_df.dropna()
    final_train_df.loc[:, 'label'] = final_train_df.loc[:, 'label'].map(int)
    final_train_df = final_train_df.reset_index(drop=True)
    return final_train_df


xx=pd.DataFrame({'Stand_text':['a']*5+['h']*5,'Simi_text':list('bcdef')+list('ijkmn')})
logger.info(f'xx:\n{xx}')

#extra_file='data/severe_simi_neg_extra_corpus.csv'
extra_file=None
train_df=build_sample_df(xx,extra_file)
logger.info(f'train_df:\n{train_df}')

###
data_file='data/simi_data/Med_general_norm_NO_1p7w_220929.xlsx'
data_df=pd.read_excel(data_file)
logger.info(f'Raw label distribution:\n{data_df.label.value_counts()}')
train_df=build_sample_df(data_df,extra_file)
logger.info(f'label distribution:\n{train_df.label.value_counts()}')


train_df_unique=train_df.drop_duplicates()
logger.info(f'label unique distribution:\n{train_df_unique.label.value_counts()}')


def train_val_test(train_data_df,label_col="label", mode_index=False, balanced=True, dev_size=0.1, test_size=0.2, seed=666):
    """
    åˆ‡åˆ†è®­ç»ƒè¯­æ–™
    :param train_data_df: DataFrame,è¦æ±‚ç´¢å¼•æ˜¯é¡ºåºçš„(Index[0,1,2,...])
    :param mode_index:æ˜¯å¦åªè¿”å›DataFrameç´¢å¼•ã€‚
    :param balanced:æ˜¯å¦é‡‡ç”¨å‘ä¸Šå‡è¡¡æŠ½æ ·ã€‚
    :param val_size:å¼€å‘é›†å¤§å°ï¼Œå¯ä»¥ä¸º0ã€‚
    :param test_size:æµ‹è¯•é›†å¤§å°ï¼Œå¯ä»¥ä¸º0ã€‚
    return: train_df,dev_df,test_df
    """
    train_data_df = train_data_df.reset_index(drop=True)
    ## å…ˆè§£å†³test_size
    if test_size > 0.:
        train_id_pre, test_idx = train_test_split(range(train_data_df.shape[0]), \
                                                   test_size=test_size, random_state=seed)
    else:
        train_id_pre, test_idx = list(range(train_data_df.shape[0])), []
    logger.info('pre train size : {} test size: {}'.format(len(train_id_pre), len(test_idx)))
    train_data_pre = train_data_df.loc[train_id_pre, :]
    test_data = train_data_df.loc[test_idx, :]
    ## å†è§£å†³dev_size
    dev_size = round(dev_size / (1 - test_size), 2)
    if dev_size > 0.:
        x_tra_id_p,x_dev_idx,y_tra_lab_p,y_dev_lab=train_test_split(train_id_pre,train_data_pre.loc[train_id_pre,label_col],
                                                                   test_size=dev_size,random_state=seed)
    else:
        x_tra_id_p,x_dev_idx,y_tra_lab_p,y_dev_lab = train_id_pre,[],train_data_pre.loc[train_id_pre,label_col],[]
    x_tra_id_pre_arr = np.array(x_tra_id_p).reshape(-1, 1)
    if balanced:
        x_tra_id_balanced,y_tra_label_balanced=RandomOverSampler(random_state=seed).fit_resample(x_tra_id_pre_arr,
                                                                                             y_tra_lab_p)
        x_tra_idx = [x[0] for x in x_tra_id_balanced]
    else:
        x_tra_idx=x_tra_id_p
    train_idx, dev_idx = x_tra_idx, x_dev_idx
    logger.info('pro train size : {}  dev size : {}  test size: {}'.format(len(train_idx), len(dev_idx), len(test_idx)))
    train_data = train_data_pre.loc[train_idx, :]
    dev_data = train_data_pre.loc[dev_idx, :]
    if mode_index:
        return train_idx, dev_idx, test_idx
    else:
        return train_data, dev_data, test_data

logger.info('Raw data shape: {}'.format(data_df.shape))
logger.info('Raw label dist:\n{}'.format(data_df.label.value_counts()))
train_data, dev_data, test_data=train_val_test(data_df,dev_size=0.1, test_size=0.1)
logger.info('train_data shape: {}'.format(train_data.shape))
logger.info('train_data label dist:\n{}'.format(train_data.label.value_counts()))
logger.info(f'train_data:\n{train_data.head()}')
logger.info(f'dev_data:\n{dev_data.head()}')
logger.info(f'test_data:\n{test_data.head()}')



##########################################################
##  Method-2: Outer-Exclusive
##########################################################
""" 
Method-2.å¤–äº’æ–¥æ­£è´Ÿæ ·æœ¬æŠ½æ ·æˆå¯¹é€»è¾‘:
    æ­£æ ·æœ¬é€»è¾‘
    M2-1.1 è‡ªæŠ½æ ·å¤åˆ¶æ‰©å……æ­£æ ·æœ¬å¯¹æ•°é‡[3æ¬¡ä¸åŒéšæœºæ“ä½œ]ã€‚
    M2-1.2 æ­£ç›¸ä¼¼é—®ä¹‹é—´æ„é€ æ­£æ ·æœ¬[åŸå§‹æ ·æœ¬é‡1.5å€]
    M2-1.3 
    è´Ÿæ ·æœ¬é€»è¾‘ï¼š
    M2-2.1 [ä¸åŒç§ç±»]-æ ‡å‡†é—®ä¹‹é—´æ„é€ è´Ÿæ ·æœ¬å¯¹,åŒ…å«ä¸å…¶ä»–æ‰€æœ‰é‡å¤ç¬›å¡å°”å’Œå»é‡æ ‡å‡†é—®ç¬›å¡å°”æˆå¯¹ï¼Œæ„æˆè´Ÿæ ·æœ¬å¯¹
    M2-2.2 [ä¸åŒç§ç±»]-æ ‡å‡†é—®çš„ç›¸ä¼¼é—®æ„é€ è´Ÿæ ·æœ¬å¯¹[é»˜è®¤é‡å¤10æ¬¡ï¼›2ä¸ªæ ·æœ¬é›†]
    M2-2.3 é¢å¤–æ‰©å±•æ•°æ®é›†(å‡è®¾ä¸ºæ ‡å‡†é—®)ä¸ç›¸ä¼¼é—®æ„é€ è´Ÿæ ·æœ¬[æ­£è´Ÿæ ·æœ¬å·®é¢é‡]
    M2-2.4 é¢å¤–æ‰©å±•æ•°æ®é›†(å‡è®¾ä¸ºç›¸ä¼¼é—®)ä¸å…¶ä»–æ ‡å‡†é—®,æ„é€ è´Ÿæ ·æœ¬å¯¹[åŸå§‹æ•°æ®é‡]
    M2-2.5 
æœ€åæ¯ç§æŠ½æ ·æ ‡è®°ğŸ“ŒæŠ½æ ·æ–¹æ³•ç¼–å·

Input :Category,Stand_text,Simi_textä¸ºæ­£æ ·æœ¬æ•°æ®df
output:Stand_text,Simi_text,label,typeå…±4ä¸ªå­—æ®µ
"""
""" 
keep method: 1. data_split_list; 2. get_positive_pair; 3.get_pair_df
"""

def Outer_Exc_get_pair_df(cate_2_simi_dict, force_simi_comb=True):
    """
    ä¸åŒç§ç±»ä¸­ç›¸ä¼¼é—®ä¹‹é—´,æ„æˆè´Ÿæ ·æœ¬å¯¹ï¼›æ­£ç›¸ä¼¼é—®ä¹‹é—´ï¼Œæ„æˆæ­£æ ·æœ¬å¯¹
    :param query_dict: {'Category':[List[Cate-Simi_text],List[Stand-Simi_text]]},é’ˆå¯¹[æ ‡å‡†é—®-æ­£ç›¸ä¼¼é—®åˆ—è¡¨]å­—å…¸
    :return:ä¸åŒç§ç±»-ç›¸ä¼¼é—®ä¹‹é—´è´Ÿæ ·æœ¬-df;ç›¸ä¼¼é—®ä¹‹é—´æ­£æ ·æœ¬-df;{'Category':[List[rest-Cate-Simi_text],List[rest-Stand-Simi_text]]}
    """
    ## å­˜å‚¨ä¸åŒç§ç±»ä¸­çš„ç›¸ä¼¼é—®ï¼Œæ„æˆè´Ÿæ ·æœ¬
    cate_simi_text_list = []
    ## ç›¸ä¼¼é—®ä¹‹é—´ï¼Œæ„æˆæ­£æ ·æœ¬
    pos_simi_text_list = []
    ## ç§ç±»-[ç›¸ä¼¼é—®List,æ ‡å‡†é—®-ç›¸ä¼¼é—®List]ï¼Œæ„æˆå­—å…¸
    cate_rest_simi_dict = {}
    for sub_category, sub_cate_cont in cate_2_simi_dict.items():
        sub_cate_simi_text_list = sub_cate_cont[0]
        sub_stand_simi_block_list = sub_cate_cont[1]
        rand_simi_text, rest_simi_text_list = data_split_list(sub_cate_simi_text_list)
        if rand_simi_text == None:
            continue
        cate_simi_text_list.append(rand_simi_text)
        sub_stand_simi_block_list_new = []
        for sub_stand_simi_list in sub_stand_simi_block_list:
            if force_simi_comb:
                ## å¼ºåˆ¶ç›¸ä¼¼æ ·æœ¬ç»„åˆæˆå¯¹
                rest_simi_text_pair_list = get_positive_pair(sub_stand_simi_list)
                pos_simi_text_list.extend(rest_simi_text_pair_list)
            elif len(sub_stand_simi_list) // 2:
                ##éšæœºé€»è¾‘ï¼Œå½“ä¸ºå¥‡æ•°æ—¶ï¼Œç›¸ä¼¼æ ·æœ¬ç»„åˆæˆå¯¹
                rest_simi_text_pair_list = get_positive_pair(sub_stand_simi_list)
                pos_simi_text_list.extend(rest_simi_text_pair_list)
            ## ç›¸ä¼¼æ ·æœ¬ä¸ç»„åˆæˆå¯¹
            ## åŒæ ·éšæœºåˆ é™¤æ­£ç›¸ä¼¼é—®ä¸€ä¸ªå…ƒç´ 
            _, sub_stand_simi_list2 = data_split_list(sub_stand_simi_list)
            sub_stand_simi_block_list_new.append(sub_stand_simi_list2)
        cate_rest_simi_dict.update({sub_category: [rest_simi_text_list, sub_stand_simi_block_list_new]})
    ## ä¸åŒç§ç±»çš„è´Ÿæ ·æœ¬ï¼Œæœ€ç»ˆè´Ÿæ ·æœ¬ï¼Œ
    neg_cate_pair_list = get_positive_pair(cate_simi_text_list)
    return pd.DataFrame(neg_cate_pair_list), pd.DataFrame(pos_simi_text_list), cate_rest_simi_dict


def Outer_Exc_sample_df_pre(data_df):
    """
    ä¸åŒç§ç±»-æ ‡å‡†é—®çš„ç›¸ä¼¼é—®ï¼Œè´Ÿæ ·æœ¬ï¼›æ ‡å‡†é—®-æ­£ç›¸ä¼¼é—®ä¹‹é—´ï¼Œæ­£æ ·æœ¬
    :param: DataFrame,åŸå§‹æ•°æ®ã€‚
    return: train-df,åŸºäºä¸åŒæ ‡å‡†é—®çš„ç›¸ä¼¼é—®æ„é€ è´Ÿæ ·æœ¬-df,ç›¸ä¼¼é—®æ­£æ ·æœ¬-df,ç»„åˆæœ€ç»ˆdf
    """
    data = data_df.copy()
    ## {ç§ç±»:[ç§ç±»ä¸‹é¢æ‰€æœ‰ç›¸ä¼¼é›†Listï¼Œ[æ ‡å‡†é—®ä¸‹é¢çš„ç›¸ä¼¼é—®List]]
    cate_2_simi_dict = {}
    ## å­˜å‚¨ç§ç±»ä¸‹ç›¸ä¼¼æ ·æœ¬æ•°é‡
    cate_simi_sample_nums_list = []
    for cate_group_item in data.groupby(['Category']):
        cate_stand_tuple = cate_group_item[0]
        stand_text_list = cate_group_item[1]['Stand_text'].tolist()
        ## é’ˆå¯¹ç§ç±»-ç›¸ä¼¼é—®ï¼Œæ„é€ è´Ÿæ ·æœ¬å¯¹
        cate_simi_text_list = cate_group_item[1]['Simi_text'].tolist()
        for sub_stand_text in stand_text_list:
            if sub_stand_text in cate_simi_text_list:
                pass
            else:
                cate_simi_text_list.append(sub_stand_text)
        cate_simi_sample_nums_list.append(len(cate_simi_text_list))
        ## ä¸åŒæ ‡å‡†é—®å¯¹åº”çš„ç›¸ä¼¼é—®ï¼Œç”¨äºæ­£æ ·æœ¬å¯¹
        stand_simi_block_list = []
        for stand_group_item in cate_group_item[1].groupby(['Stand_text']):
            simi_text_list = stand_group_item[1]['Simi_text'].tolist()
            stand_simi_block_list.append(simi_text_list)
        cate_2_simi_dict.update({cate_stand_tuple: [cate_simi_text_list, stand_simi_block_list]})
    cate_simi_new_dict = cate_2_simi_dict.copy()
    neg_stand0simi_pair_df = []
    pos_stand0simi_pair_df = []
    ## é‡å¤10æ¬¡;æˆ–è€…
    ## åŒæ—¶ä¿ç•™æ­£è´Ÿæ ·æœ¬å¯¹ã€‚
    loop_nums=pd.Series(cate_simi_sample_nums_list).describe()['25%']
    loop_nums=int(loop_nums)
    logger.info(f'Outer Simi Loop nums: {loop_nums}')
    for i in range(loop_nums):
        sub_neg_stand0simi_df, sub_pos_simi_df, stand_simi_new_dict = Outer_Exc_get_pair_df(cate_simi_new_dict,force_simi_comb=False)
        neg_stand0simi_pair_df.append(sub_neg_stand0simi_df)
        pos_stand0simi_pair_df.append(sub_pos_simi_df)
    sub_neg_stand0simi_df, sub_pos_simi_df, stand_simi_new_dict = Outer_Exc_get_pair_df(cate_simi_new_dict,force_simi_comb=True)
    neg_stand0simi_pair_df.append(sub_neg_stand0simi_df)
    pos_stand0simi_pair_df.append(sub_pos_simi_df)
    neg_stand_simi_pair_df = pd.concat(neg_stand0simi_pair_df, axis=0,sort=False)
    pos_stand_simi_pair_df = pd.concat(pos_stand0simi_pair_df, axis=0, sort=False)
    neg_stand_simi_pair_df['label'] = 0
    pos_stand_simi_pair_df['label'] = 1
    train_df = pd.concat([neg_stand_simi_pair_df, pos_stand_simi_pair_df], axis=0,sort=False).reset_index(drop=True)
    train_df.columns=['Stand_text', 'Simi_text', 'label']
    train_df['type']='Outer_Simi'
    train_df = train_df.sample(frac=1).reset_index(drop=True)
    return train_df

#### M2-Outer Dev
## {ç§ç±»:[ç§ç±»ä¸‹é¢æ‰€æœ‰ç›¸ä¼¼é›†Listï¼Œ[æ ‡å‡†é—®ä¸‹é¢çš„ç›¸ä¼¼é—®List]]
cate_2_simi_dict = {}
## å­˜å‚¨ç§ç±»ä¸‹ç›¸ä¼¼æ ·æœ¬æ•°é‡
cate_simi_sample_nums_list=[]
for cate_group_item in data_df.head(100).groupby(['Category']):
    cate_stand_tuple = cate_group_item[0]
    stand_text_list = cate_group_item[1]['Stand_text'].tolist()
    ## é’ˆå¯¹ç§ç±»-ç›¸ä¼¼é—®ï¼Œæ„é€ è´Ÿæ ·æœ¬å¯¹
    cate_simi_text_list=cate_group_item[1]['Simi_text'].tolist()
    for sub_stand_text in stand_text_list:
        if sub_stand_text in cate_simi_text_list:
            pass
        else:
            cate_simi_text_list.append(sub_stand_text)
    cate_simi_sample_nums_list.append(len(cate_simi_text_list))
    ## ä¸åŒæ ‡å‡†é—®å¯¹åº”çš„ç›¸ä¼¼é—®ï¼Œç”¨äºæ­£æ ·æœ¬å¯¹
    stand_simi_block_list=[]
    for stand_group_item in cate_group_item[1].groupby(['Stand_text']):
        simi_text_list=stand_group_item[1]['Simi_text'].tolist()
        stand_simi_block_list.append(simi_text_list)
    cate_2_simi_dict.update({cate_stand_tuple:[cate_simi_text_list,stand_simi_block_list]})

pprint(cate_2_simi_dict)
print(pd.Series(cate_simi_sample_nums_list).describe())
print(pd.Series(cate_simi_sample_nums_list).describe()['25%'])

## å­˜å‚¨ä¸åŒç§ç±»ä¸­çš„ç›¸ä¼¼é—®ï¼Œæ„æˆè´Ÿæ ·æœ¬
cate_simi_text_list = []
## ç›¸ä¼¼é—®ä¹‹é—´ï¼Œæ„æˆæ­£æ ·æœ¬
pos_simi_text_list = []
## ç§ç±»-[ç›¸ä¼¼é—®List,æ ‡å‡†é—®-ç›¸ä¼¼é—®List]ï¼Œæ„æˆå­—å…¸
cate_rest_simi_dict = {}
force_simi_comb=False
for sub_category, sub_cate_cont in cate_2_simi_dict.items():
    sub_cate_simi_text_list=sub_cate_cont[0]
    sub_stand_simi_block_list=sub_cate_cont[1]
    rand_simi_text, rest_simi_text_list = data_split_list(sub_cate_simi_text_list)
    if rand_simi_text == None:
        continue
    cate_simi_text_list.append(rand_simi_text)
    sub_stand_simi_block_list_new=[]
    for sub_stand_simi_list in sub_stand_simi_block_list:
        if force_simi_comb:
            ## å¼ºåˆ¶ç›¸ä¼¼æ ·æœ¬ç»„åˆæˆå¯¹
            rest_simi_text_pair_list = get_positive_pair(sub_stand_simi_list)
            pos_simi_text_list.extend(rest_simi_text_pair_list)
        elif len(sub_stand_simi_list) // 2:
            ##éšæœºé€»è¾‘ï¼Œå½“ä¸ºå¥‡æ•°æ—¶ï¼Œç›¸ä¼¼æ ·æœ¬ç»„åˆæˆå¯¹
            rest_simi_text_pair_list = get_positive_pair(sub_stand_simi_list)
            pos_simi_text_list.extend(rest_simi_text_pair_list)
        ## ç›¸ä¼¼æ ·æœ¬ä¸ç»„åˆæˆå¯¹
        ## åŒæ ·éšæœºåˆ é™¤æ­£ç›¸ä¼¼é—®ä¸€ä¸ªå…ƒç´ 
        _,sub_stand_simi_list2=data_split_list(sub_stand_simi_list)
        sub_stand_simi_block_list_new.append(sub_stand_simi_list2)
    cate_rest_simi_dict.update({sub_category: [rest_simi_text_list,sub_stand_simi_block_list_new]})
## ä¸åŒç§ç±»çš„è´Ÿæ ·æœ¬ï¼Œæœ€ç»ˆè´Ÿæ ·æœ¬ï¼Œ
neg_cate_pair_list = get_positive_pair(cate_simi_text_list)

neg_cate_simi_df=pd.DataFrame(neg_cate_pair_list)
pos_stand_simi_df=pd.DataFrame(pos_simi_text_list)

logger.info(f'neg_cate_simi_df:\n{neg_cate_simi_df.head()}')
logger.info(f'pos_stand_simi_df:\n{pos_stand_simi_df.head()}')

#### Test

xx=pd.DataFrame({'Category':['A']*5+['H']*5,'Stand_text':['a1']*3+['a2']*2+['h1']*3+['h2']*2,
                 'Simi_text':list('bcdef')+list('ijkmn')})
logger.info(f'xx:\n{xx}')

train_df=Outer_Exc_sample_df_pre(xx)
logger.info(f'train_df:\n{train_df}')


def Outer_Exc_sample_focus_simi_neg_df(data_df,extra_file):
    """
    é‡ç‚¹åŸºäºç§ç±»-ç›¸ä¼¼è¯­æ–™æ„é€ è´Ÿæ ·æœ¬
    1. åŸºäºåŸºäºä¸åŒç§ç±»çš„ç›¸ä¼¼é—®æ„é€ è´Ÿæ ·æœ¬-dfçš„base-method,ç”Ÿæˆdf
    2. é¢å¤–æ ·æœ¬[å‡è®¾ä¸ºæ ‡å‡†é—®]ä¸ç›¸ä¼¼æ ·æœ¬[åŸå§‹ç›¸ä¼¼]ï¼Œæ„é€ è´Ÿæ ·æœ¬é›†ã€‚
    :param data_df:DataFrame,åŸå§‹æ•°æ®ã€‚
    :param extra_file:é¢å¤–æ‰©å±•æ–‡ä»¶ï¼Œcsvæ ¼å¼
    return:train-df,åŸºäº1ï¼Œ2ä¸¤ç§æ€è·¯æ„é€ è´Ÿæ ·æœ¬è®­ç»ƒé›†ã€‚
    """
    train_df1 = Outer_Exc_sample_df_pre(data_df)
    train_df2 = Outer_Exc_sample_df_pre(data_df)
    train_data_pre = pd.concat([train_df1, train_df2], axis=0)
    label_dist = train_data_pre.label.value_counts()
    if label_dist.shape[0]>1:
        diff = label_dist[1] - label_dist[0]
        #æ­£è´Ÿæ ·æœ¬æ•°é‡å·®é¢
        diff = abs(diff)
        #print('diff: ',diff)
        data = data_df.copy()
        simi_text_series = data.loc[:, 'Simi_text'].sample(diff, replace=True)
    else:
        diff=int(label_dist/2)
        simi_text_series = train_data_pre.loc[:, 'Simi_text'].sample(diff)
    if extra_file:
        corpus_extra = pd.read_csv(extra_file, header=None)
        logger.info('Extra  data shape: {}'.format(corpus_extra.shape))
        if diff < corpus_extra.shape[0]:
            extra_neg_text = corpus_extra.sample(diff).loc[:, 0]
        else:
            extra_neg_text = corpus_extra.sample(diff, replace=True).loc[:, 0]
        extra_neg_df = pd.DataFrame({'Simi_text': simi_text_series.tolist(), 'Stand_text': extra_neg_text.tolist(), 'label': 0})
        extra_neg_df = extra_neg_df.loc[:, ['Stand_text', 'Simi_text', 'label']]
        extra_neg_df['type']='Extra_Simi'
        ## è¿›ä¸€æ­¥å¢åŠ é¢å¤–æ ·æœ¬ä¸ç›¸ä¼¼é›†æ„é€ è´Ÿæ ·æœ¬å¯¹ï¼Œæ•°é‡ä¸ºåŸå§‹æ•°æ®é›†ä¸€åŠ
        further_sample_nums=int(data_df.shape[0]*0.5)
        simi_raw_text_series=data_df.loc[:, 'Simi_text'].sample(further_sample_nums)
        further_extra_neg_series=corpus_extra.sample(further_sample_nums).loc[:, 0]
        extra_neg_df2 = pd.DataFrame(
            {'Simi_text': simi_raw_text_series.tolist(), 'Stand_text': further_extra_neg_series.tolist(), 'label': 0})
        extra_neg_df2 = extra_neg_df2.loc[:, ['Stand_text', 'Simi_text', 'label']]
        extra_neg_df2['type'] = 'Extra_Simi'
        all_data = pd.concat([train_data_pre, extra_neg_df2], axis=0,sort=False)
    else:
        all_data=train_data_pre
    all_data = all_data.sample(frac=1)
    return all_data

#### Test
xx=pd.DataFrame({'Category':['A']*5+['H']*5,'Stand_text':['a1']*3+['a2']*2+['h1']*3+['h2']*2,
                 'Simi_text':list('bcdef')+list('ijkmn')})
logger.info(f'xx:\n{xx}')

#extra_file='data/severe_simi_neg_extra_corpus.csv'
extra_file=None
train_df=Outer_Exc_sample_focus_simi_neg_df(xx,extra_file)
logger.info(f'train_df:\n{train_df}')


def Outer_Exc_build_sample_df(data_df,extra_file):
    """
    1. æ­£æ ·æœ¬è‡ªæŠ½æ ·ï¼Œæ‰©å……æ­£æ ·æœ¬æ•°é‡,æ„é€ æ­£æ ·æœ¬å¯¹
    2. é¢å¤–æ‰©å±•æ ·æœ¬ä¸å…¶ä»–æ ‡å‡†é—®,æ„é€ è´Ÿæ ·æœ¬å¯¹
    3. å½“å‰æ ‡å‡†é—®ä¸å…¶ä»–æ ‡å‡†é—®ï¼ŒåŒ…å«ä¸å…¶ä»–æ‰€æœ‰é‡å¤ç¬›å¡å°”å’Œå»é‡æ ‡å‡†é—®ç¬›å¡å°”æˆå¯¹ï¼Œæ„æˆè´Ÿæ ·æœ¬å¯¹
    :param data_df: åªæœ‰æ­£æ ·æœ¬çš„åŸå§‹df,å¿…é¡»åŒ…å«Category,Stand_text,Simi_text,å…±3ä¸ªå­—æ®µ
    :param extra_file: é¢å¤–æ‰©å±•æ–‡ä»¶ï¼Œcsvæ ¼å¼
    return DataFrame: æŠ½æ ·ç»„æˆæœ€ç»ˆdf,å¿…é¡»åŒ…å«Category,Stand_text,Simi_text,labelå…±4ä¸ªå­—æ®µ
    """
    ## 1. æ­£æ ·æœ¬è‡ªæŠ½æ ·ï¼Œæ‰©å……æ­£æ ·æœ¬æ•°é‡
    data = data_df.copy()
    df_cols=['Stand_text','Simi_text','label','type']
    # è‡ªæŠ½æ ·æ‰©å……æ­£æ ·æœ¬æ•°é‡
    data_pos_sample_df1 = data.sample(frac=0.5)
    data_pos_sample_df2 = data.sample(frac=1.5, replace=True)
    data_pos_sample_df3 = data.loc[:, ['Category','Simi_text', 'Stand_text']].sample(frac=0.5)
    data_pos_sample_df3.columns=['Category','Stand_text','Simi_text']
    standard_query = data['Stand_text'].unique().tolist()
    # 2. é¢å¤–æ‰©å±•æ ·æœ¬ä¸å…¶ä»–æ ‡å‡†é—®ï¼Œæ„é€ è´Ÿæ ·æœ¬ã€‚
    if extra_file:
        data_extra = pd.read_csv(extra_file, header=None)
        data_extra = data_extra[0].unique().tolist()
        ## æ ‡å‡†é—®ä¸é¢å¤–æ ·æœ¬ç¬›å¡å°”ç§¯è·å¾—è´Ÿæ ·æœ¬å¯¹
        data_neg_extra_df4 = pd.DataFrame(list(itertools.product(standard_query, data_extra)))
        data_neg_extra_df4 = data_neg_extra_df4.sample(data.shape[0])
        data_neg_extra_df4.columns = ['Stand_text', 'Simi_text']
        data_neg_extra_df4['label']=0
        data_neg_extra_df4['type'] = 'Extra_Stand'
        data_neg_extra_df4=data_neg_extra_df4.loc[:,df_cols]
    else:
        data_neg_extra_df4=pd.DataFrame()
    #3.1 å½“å‰æ ‡å‡†é—®ä¸å…¶ä»–æ ‡å‡†é—®ï¼Œä¸å…¶ä»–æ‰€æœ‰é‡å¤æ ‡å‡†é—®ç¬›å¡å°”
    neg_stand_pair_list = []
    for sub_stand in standard_query:
        data2 = data[data['Stand_text'] != sub_stand]
        other_stand = data2['Stand_text'].tolist()
        neg_stand_pair_list.extend(list(itertools.product([sub_stand], other_stand)))
    data_neg_stand_df5 = pd.DataFrame(neg_stand_pair_list)
    data_neg_stand_df5 = data_neg_stand_df5.sample(data.shape[0])
    #3.2 æ ‡å‡†é—®ä¹‹é—´å»é‡æ ‡å‡†é—®ç¬›å¡å°”æˆå¯¹
    stand_unique_list = data['Stand_text'].unique().tolist()
    data_neg_stand_df6_pre = pd.DataFrame(list(itertools.combinations(stand_unique_list, 2)))
    if data_neg_stand_df6_pre.shape[0] > int(data.shape[0] / 2):
        data_neg_stand_df6 = data_neg_stand_df6_pre.sample(int(len(data) / 2))
    else:
        data_neg_stand_df6 = data_neg_stand_df6_pre.sample(int(len(data) / 2), replace=True)
    sample_pos_df = pd.concat([data_pos_sample_df1, data_pos_sample_df2, data_pos_sample_df3],axis=0,sort=False)
    sample_pos_df['label'] = 1
    sample_pos_df['type']='Self_Simi'
    sample_pos_df=sample_pos_df.loc[:,df_cols]
    sample_neg_df=pd.concat([data_neg_stand_df5, data_neg_stand_df6],axis=0,sort=False)
    sample_neg_df.columns=['Stand_text','Simi_text']
    sample_neg_df['label']=0
    sample_neg_df['type'] = 'Stand_Inner'
    sample_neg_df = sample_neg_df.loc[:, df_cols]
    #é‡ç‚¹åŸºäºç›¸ä¼¼é—®æ„é€ è´Ÿæ ·æœ¬å’Œä¸€å®šé‡çš„æ­£æ ·æœ¬
    train_focus_neg_df = Outer_Exc_sample_focus_simi_neg_df(data_df,extra_file)
    train_focus_neg_df = train_focus_neg_df.sample(len(data))
    logger.info(f'sample_pos_df:\n{sample_pos_df.head()}')
    logger.info(f'data_neg_extra_df4:\n{data_neg_extra_df4.head()}')
    logger.info(f'sample_neg_df:\n{sample_neg_df.head()}')
    logger.info(f'train_focus_neg_df:\n{train_focus_neg_df.head()}')
    final_train_df = pd.concat([sample_pos_df,data_neg_extra_df4,sample_neg_df,train_focus_neg_df], axis=0, sort=False)
    final_train_df = final_train_df.sample(frac=1)
    final_train_df = final_train_df.dropna()
    final_train_df.loc[:, 'label'] = final_train_df.loc[:, 'label'].map(int)
    final_train_df = final_train_df.reset_index(drop=True)
    return final_train_df

#### Test
xx=pd.DataFrame({'Category':['A']*5+['H']*5,'Stand_text':['a1']*3+['a2']*2+['h1']*3+['h2']*2,
                 'Simi_text':list('bcdef')+list('ijkmn')})
logger.info(f'xx:\n{xx}')

#extra_file='data/simi_data/severe_simi_neg_extra_corpus.csv'
extra_file=None
train_df=Outer_Exc_build_sample_df(xx,extra_file)
logger.info(f'train_df:\n{train_df}')


##########################
##  M-2: Data-Augment
##########################
""" 
DA-strategy
1.ä¸»è¦ä»¥åŠ å…¥æ ‡ç‚¹ç¬¦å·ä¸ºä¸»ï¼Œéšæœºåˆ æ‰ä¸ºè¾…ã€‚
2.åªé’ˆå¯¹æ­£æ ·æœ¬ç›¸ä¼¼é›†å¤„ç†ï¼›åªé’ˆå¯¹å­—ç¬¦ä¸²é•¿åº¦>=3çš„è¿›è¡Œæ“ä½œã€‚
  insert-punctuation:0.8
  random_delete:0.1 
  keep_original:0.1
"""
"""
EDA-æ–¹æ³•
æå‡ºäº†å››ç§ç®€å•çš„æ•°æ®å¢å¼ºæ“ä½œï¼ŒåŒ…æ‹¬ï¼š
1.åŒä¹‰è¯æ›¿æ¢(Synonym Replacement, SR)ï¼šä»å¥å­ä¸­éšæœºé€‰å–nä¸ªä¸å±äºåœç”¨è¯é›†çš„å•è¯ï¼Œå¹¶éšæœºé€‰æ‹©å…¶åŒä¹‰è¯æ›¿æ¢å®ƒä»¬ï¼›
2.éšæœºæ’å…¥(Random Insertion, RI)ï¼šéšæœºçš„æ‰¾å‡ºå¥ä¸­æŸä¸ªä¸å±äºåœç”¨è¯é›†çš„è¯ï¼Œå¹¶æ±‚å‡ºå…¶éšæœºçš„åŒä¹‰è¯ï¼Œå°†è¯¥åŒä¹‰è¯æ’å…¥å¥å­çš„ä¸€ä¸ªéšæœºä½ç½®ã€‚é‡å¤næ¬¡ï¼›
3.éšæœºäº¤æ¢(Random Swap, RS)ï¼šéšæœºçš„é€‰æ‹©å¥ä¸­ä¸¤ä¸ªå•è¯å¹¶äº¤æ¢å®ƒä»¬çš„ä½ç½®ã€‚é‡å¤næ¬¡ï¼›
4.éšæœºåˆ é™¤(Random Deletion, RD)ï¼šä»¥  çš„æ¦‚ç‡ï¼Œéšæœºçš„ç§»é™¤å¥ä¸­çš„æ¯ä¸ªå•è¯ï¼›
5.åŒä¹‰è¯æ›¿æ¢ä¸ä½¿ç”¨è¯è¡¨ï¼Œè€Œæ˜¯ä½¿ç”¨è¯å‘é‡æˆ–è€…é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼›
6.é€šè¿‡åœ¨åœ¨æ–‡æœ¬ä¸­æ’å…¥ä¸€äº›ç¬¦åˆæˆ–è€…è¯è¯­ï¼Œæ¥å¢åŠ å™ªå£°ï¼›å¦‚åŸå§‹æ–‡æœ¬ä¸­éšæœºæ’å…¥ä¸€äº›æ ‡ç‚¹ç¬¦å·
7.å°†å¥å­é€šè¿‡ç¿»è¯‘å™¨ç¿»è¯‘æˆå¦å¤–ä¸€ç§è¯­è¨€å†ç¿»è¯‘å›æ¥çš„å›è¯‘æ‰‹æ®µ
"""
"""
(AEDA)An Easier Data Augmentation Technique for Text Classification
å¢åŠ å™ªå£°ï¼›å¦‚åŸå§‹æ–‡æœ¬ä¸­éšæœºæ’å…¥ä¸€äº›æ ‡ç‚¹ç¬¦å·
Qï¼šæ’å…¥å¤šå°‘æ ‡ç‚¹ç¬¦å·ï¼ŸAï¼šä»1åˆ°ä¸‰åˆ†ä¹‹ä¸€å¥å­é•¿åº¦ä¸­ï¼Œéšæœºé€‰æ‹©ä¸€ä¸ªæ•°ï¼Œä½œä¸ºæ’å…¥æ ‡ç‚¹ç¬¦å·çš„ä¸ªæ•°ã€‚
Qï¼šä¸ºä»€ä¹ˆæ˜¯1åˆ°ä¸‰åˆ†ä¹‹ä¸€å¥é•¿ï¼ŸAï¼šä½œè€…è¡¨ç¤ºï¼Œæ—¢æƒ³æ¯ä¸ªå¥å­ä¸­æœ‰æ ‡ç‚¹ç¬¦å·æ’å…¥ï¼Œå¢åŠ å¥å­çš„å¤æ‚æ€§ï¼›åˆä¸æƒ³åŠ å…¥å¤ªå¤šæ ‡ç‚¹ç¬¦å·ï¼Œ
                         è¿‡äºå¹²æ‰°å¥å­çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶ä¸”å¤ªå¤šå™ªå£°å¯¹æ¨¡å‹å¯èƒ½æœ‰è´Ÿé¢å½±å“ã€‚
Qï¼šå¥å­æ’å…¥æ ‡ç‚¹ç¬¦å·çš„ä½ç½®å¦‚ä½•é€‰å–ï¼ŸAï¼šéšæœºæ’å…¥ã€‚
Qï¼šæ ‡ç‚¹ç¬¦å·å…±åŒ…å«å“ªäº›ï¼ŸAï¼šä¸»è¦æœ‰6ç§ï¼Œâ€œ.â€ã€â€œ;â€ã€â€œ?â€ã€â€œ:â€ã€â€œ!â€ã€â€œ,â€ã€‚
Qï¼šAEDAæ¯”EDAæ•ˆæœå¥½çš„ç†è®ºåŸºç¡€æ˜¯ä»€ä¹ˆï¼ŸAï¼šä½œè€…è®¤ä¸ºï¼ŒEDAæ–¹æ³•ï¼Œå¦‚è®ºæ˜¯åŒä¹‰è¯æ›¿æ¢ï¼Œè¿˜æ˜¯éšæœºæ›¿æ¢ã€éšæœºæ’å…¥ã€éšæœºåˆ é™¤ï¼Œéƒ½æ”¹å˜äº†åŸå§‹æ–‡æœ¬çš„åºåˆ—ä¿¡æ¯ï¼›
                               è€ŒAEDAæ–¹æ³•ï¼Œåªæ˜¯æ’å…¥æ ‡ç‚¹ç¬¦å·ï¼Œå¯¹äºåŸå§‹æ•°æ®çš„åºåˆ—ä¿¡æ¯ä¿®æ”¹ä¸æ˜æ˜¾ã€‚ä¸ªäººç†è§£ï¼Œé€šè¿‡è¯è¯­ä¿®æ”¹çš„æ–¹æ³•ï¼Œ
                               ä¸åŸå§‹è¯­ä¹‰æ”¹å˜å¯ä»¥æ›´åŠ è´Ÿé¢ï¼›è€Œä»…æ’å…¥ä¸€äº›æ ‡ç‚¹ç¬¦å·ï¼Œè™½ç„¶å¢åŠ äº†å™ªå£°ï¼Œä½†æ˜¯åŸå§‹æ–‡æœ¬çš„è¯­åºå¹¶æ²¡æœ‰æ”¹å˜.
å®éªŒç»“æœï¼šå½“æ•°æ®é›†è¾ƒå°æ—¶ï¼Œæ•°æ®å¢å¼ºå€æ•°è¶Šå¤§ï¼Œæ•ˆæœæå‡çš„è¶Šæ˜æ˜¾ï¼›
        ä½†æ˜¯å½“æ•°æ®é‡è¾ƒå¤§æ—¶ï¼Œæ•°æ®å¢å¼ºå€æ•°è¶Šå¤§ï¼Œæ•ˆæœæå‡å°†ä¼šä¸‹é™ã€‚
"""


def eda_insert_punctuation(sentence, punc_ratio=0.3):
    PUNCTUATIONS = ['.', ',', '!', '?', ';', ':']
    words = list(sentence)
    new_line = []
    insert_punc_nums = random.randint(1, int(punc_ratio * len(words) + 1))
    insert_idx = random.sample(range(0, len(words)), insert_punc_nums)
    for sub_idx, word in enumerate(words):
      if sub_idx in insert_idx:
        new_line.append(PUNCTUATIONS[random.randint(0, len(PUNCTUATIONS)-1)])
        new_line.append(word)
      else:
        new_line.append(word)
    new_line = ''.join(new_line)
    return new_line

xx="ä½†æ˜¯å½“æ•°æ®é‡è¾ƒå¤§æ—¶ï¼Œæ•°æ®å¢å¼ºå€æ•°è¶Šå¤§ï¼Œæ•ˆæœæå‡å°†ä¼šä¸‹é™ã€‚"
xx_res=eda_insert_punctuation(xx)
logger.info(f'Input text: {xx}')
logger.info(f'eda_insert_punctuation Result: {xx_res}')


### éšæœºåˆ é™¤
def eda_random_delete(words, drop_proba=0.2):
    if len(words) == 1:
        return words
    new_words = []
    for word in words:
        r = random.uniform(0, 1)
        if r > drop_proba:
            new_words.append(word)
    if len(new_words) == 0:
        rand_int = random.randint(0, len(words)-1)
        return [words[rand_int]]
    return ''.join(new_words)

xx="ä½†æ˜¯å½“æ•°æ®é‡è¾ƒå¤§æ—¶ï¼Œæ•°æ®å¢å¼ºå€æ•°è¶Šå¤§ï¼Œæ•ˆæœæå‡å°†ä¼šä¸‹é™ã€‚"
xx_res=eda_random_delete(xx)
logger.info(f'              Input text: {xx}')
logger.info(f'eda_random_delete Result: {xx_res}')


def EDA_build_sampler_df(data_df,insert_proba=0.8,delete_proba=0.1):
    """
    åªé’ˆå¯¹æ­£æ ·æœ¬ç›¸ä¼¼é›†å¤„ç†ï¼›åªé’ˆå¯¹å­—ç¬¦ä¸²é•¿åº¦ >= 3çš„è¿›è¡Œæ“ä½œã€‚
    insert_punctuation: 0.8
    random_delete: 0.1
    keep_original: 0.1
    :param data_df: åªæœ‰æ­£æ ·æœ¬çš„åŸå§‹df,å¿…é¡»åŒ…å«Category,Stand_text,Simi_text,å…±3ä¸ªå­—æ®µ
    :return DataFrame: æŠ½æ ·ç»„æˆæœ€ç»ˆdf,å¿…é¡»åŒ…å«Category,Stand_text,Simi_text,labelå…±4ä¸ªå­—æ®µ
    """
    def eda_operation(text:str):
        if len(text.strip())>=3:
            rand = random.uniform(0, 1)
            if rand>1-insert_proba:
                eda_text=eda_insert_punctuation(text)
            elif rand>1-delete_proba:
                eda_text = eda_random_delete(text)
            else:
                eda_text=text
        else:
            return text
        return eda_text

    data=data_df.copy()
    data.loc[:,'eda_text']=data.loc[:,'Simi_text'].map(eda_operation)
    data=data.loc[:,['Stand_text','eda_text']]
    data.rename(columns={'eda_text':'Simi_text'},inplace=True)
    data['label']=1
    data['type'] = 'EDA'
    return data


#### Test
xx=pd.DataFrame({'Category':['A']*5+['H']*5,'Stand_text':['a1']*3+['a2']*2+['h1']*3+['h2']*2,
                 'Simi_text':list('bcd')+['adfasdfa','adfafaaad']+list('ijk')+["é•¿å¥å­ç›¸å¯¹äºçŸ­å¥å­","å­˜åœ¨ä¸€ä¸ªç‰¹æ€§"]})
logger.info(f'xx:\n{xx}')

train_df=EDA_build_sampler_df(xx)
logger.info(f'train_df:\n{train_df}')

##########################################################
##  Method-3: åˆå§‹Inner-Exclusive
##########################################################
""" 
Method-3.å†…äº’æ–¥æ­£è´Ÿæ ·æœ¬æŠ½æ ·æˆå¯¹é€»è¾‘:
    æ­£æ ·æœ¬é€»è¾‘
    M3-1.1 è‡ªæŠ½æ ·å¤åˆ¶æ‰©å……æ­£æ ·æœ¬å¯¹æ•°é‡[3æ¬¡ä¸åŒéšæœºæ“ä½œ]ä¸ºä¸»ã€‚
    M3-1.2 æ ‡å‡†é—®-æ­£ç›¸ä¼¼é—®ä¹‹é—´æ„é€ æ­£æ ·æœ¬[åŸå§‹æ ·æœ¬é‡1.5å€]ä¸ºè¾…ï¼Œå åŸå§‹æ­£æ ·æœ¬çš„25%ã€‚
    M3-1.3 
    è´Ÿæ ·æœ¬é€»è¾‘ï¼š
    M3-2.1 [ç›¸åŒç§ç±»-æ ‡å‡†é—®]ä¹‹é—´æ„é€ è´Ÿæ ·æœ¬å¯¹,åŒ…å«ä¸å…¶ä»–æ‰€æœ‰é‡å¤ç¬›å¡å°”å’Œå»é‡æ ‡å‡†é—®ç¬›å¡å°”æˆå¯¹ï¼Œæ„æˆè´Ÿæ ·æœ¬å¯¹
    M3-2.2 [ç›¸åŒç§ç±»-æ ‡å‡†é—®]çš„ç›¸ä¼¼é—®æ„é€ è´Ÿæ ·æœ¬å¯¹[é»˜è®¤é‡å¤10æ¬¡ï¼›2ä¸ªæ ·æœ¬é›†]ã€‚
    M3-2.3 é¢å¤–æ‰©å±•æ•°æ®é›†(å‡è®¾ä¸ºæ ‡å‡†é—®)ä¸ç›¸ä¼¼é—®æ„é€ è´Ÿæ ·æœ¬[æ­£è´Ÿæ ·æœ¬å·®é¢é‡]
    M3-2.4 é¢å¤–æ‰©å±•æ•°æ®é›†(å‡è®¾ä¸ºç›¸ä¼¼é—®)ä¸å…¶ä»–æ ‡å‡†é—®,æ„é€ è´Ÿæ ·æœ¬å¯¹[åŸå§‹æ•°æ®é‡]
    M3-2.5 
æœ€åæ¯ç§æŠ½æ ·æ ‡è®°ğŸ“ŒæŠ½æ ·æ–¹æ³•ç¼–å·
"""
""" 
keep method: 1. data_split_list; 2. get_positive_pair; 3.get_pair_df

## ç›®å‰åŸºäºæœ€è¿‘pandas>1.0çš„groupbyçš„ç‰¹æ€§æ›´æ”¹
"""

def Inner_Exc_get_pair_df(cate_2_simi_dict, force_simi_comb=True):
    """
    ä¸åŒç§ç±»ä¸­ç›¸ä¼¼é—®ä¹‹é—´,æ„æˆè´Ÿæ ·æœ¬å¯¹ï¼›æ­£ç›¸ä¼¼é—®ä¹‹é—´ï¼Œæ„æˆæ­£æ ·æœ¬å¯¹
    :param query_dict: {'Category':[List[Cate-Simi_text],List[Stand-Simi_text]]},é’ˆå¯¹[æ ‡å‡†é—®-æ­£ç›¸ä¼¼é—®åˆ—è¡¨]å­—å…¸
    return:ä¸åŒç§ç±»-ç›¸ä¼¼é—®ä¹‹é—´è´Ÿæ ·æœ¬-df;ç›¸ä¼¼é—®ä¹‹é—´æ­£æ ·æœ¬-df;{'Category':[List[rest-Cate-Simi_text],List[rest-Stand-Simi_text]]}
    """
    ## å­˜å‚¨ç›¸åŒç§ç±»ä¸­çš„ä¸åŒæ ‡å‡†é—®-ç›¸ä¼¼é—®ï¼Œæ„æˆè´Ÿæ ·æœ¬
    neg_diff_stand_2_simi_pair_list = []
    ## ç›¸ä¼¼é—®ä¹‹é—´ï¼Œæ„æˆæ­£æ ·æœ¬
    pos_simi_text_pair_list = []
    ## ç§ç±»-[rest-æ ‡å‡†é—®_ç›¸ä¼¼é—®List]ï¼Œæ„æˆå­—å…¸
    cate_rest_simi_dict = {}
    for sub_category, sub_stand_simi_cont in cate_2_simi_dict.items():
        sub_cate_stand_simi_block_list_new = []
        ## å­˜å‚¨ç›¸åŒç§ç±»ä¸­çš„ä¸åŒæ ‡å‡†é—®-ç›¸ä¼¼é—®ï¼Œæ„æˆè´Ÿæ ·æœ¬
        sub_stand_2_simi_text_list = []
        for sub_stand_simi_list in sub_stand_simi_cont:
            rand_simi_text, rest_simi_text_list = data_split_list(sub_stand_simi_list)
            if rand_simi_text == None:
                continue
            sub_stand_2_simi_text_list.append(rand_simi_text)
            if force_simi_comb:
                ## å¼ºåˆ¶ç›¸ä¼¼æ ·æœ¬ç»„åˆæˆå¯¹
                rest_simi_text_pair_list = get_positive_pair(rest_simi_text_list)
                pos_simi_text_pair_list.extend(rest_simi_text_pair_list)
            elif len(rest_simi_text_list) // 2:
                ##éšæœºé€»è¾‘ï¼Œå½“ä¸ºå¥‡æ•°æ—¶ï¼Œç›¸ä¼¼æ ·æœ¬ç»„åˆæˆå¯¹
                rest_simi_text_pair_list = get_positive_pair(rest_simi_text_list)
                pos_simi_text_pair_list.extend(rest_simi_text_pair_list)
            ## ç›¸ä¼¼æ ·æœ¬ä¸ç»„åˆæˆå¯¹
            sub_cate_stand_simi_block_list_new.append(rest_simi_text_list)
        cate_rest_simi_dict.update({sub_category: sub_cate_stand_simi_block_list_new})
        ## ç›¸åŒç§ç±»çš„ä¸åŒæ ‡å‡†é—®çš„ç›¸ä¼¼é—®ï¼Œæ„é€ è´Ÿæ ·æœ¬å¯¹ï¼Œ
        neg_cate_pair_list = get_positive_pair(sub_stand_2_simi_text_list)
        logger.info(f'neg_cate_pair_list: {neg_cate_pair_list}')
        neg_diff_stand_2_simi_pair_list.extend(neg_cate_pair_list)
    return pd.DataFrame(neg_diff_stand_2_simi_pair_list), pd.DataFrame(pos_simi_text_pair_list), cate_rest_simi_dict

def Inner_Exc_sample_df_pre(data_df):
    """
    ç›¸åŒç§ç±»-æ ‡å‡†é—®çš„ç›¸ä¼¼é—®ï¼Œè´Ÿæ ·æœ¬ï¼›æ ‡å‡†é—®-æ­£ç›¸ä¼¼é—®ä¹‹é—´ï¼Œæ­£æ ·æœ¬
    :param: DataFrame,åŸå§‹æ•°æ®ã€‚
    return: train-df,åŸºäºä¸åŒæ ‡å‡†é—®çš„ç›¸ä¼¼é—®æ„é€ è´Ÿæ ·æœ¬-df,ç›¸ä¼¼é—®æ­£æ ·æœ¬-df,ç»„åˆæœ€ç»ˆdf
    """
    data = data_df.copy()
    ## {ç§ç±»:[æ ‡å‡†é—®ä¸‹é¢çš„ç›¸ä¼¼é—®List,...,[]]
    cate_2_simi_dict = {}
    ## å­˜å‚¨ç§ç±»ä¸‹ç›¸ä¼¼æ ·æœ¬æ•°é‡
    cate_simi_sample_nums_list = []
    for cate_group_item in data.groupby(['Category']):
        cate_stand_tuple = cate_group_item[0][0]##æ–°ç‰ˆpandas-gropyby
        ## ä¸åŒæ ‡å‡†é—®å¯¹åº”çš„ç›¸ä¼¼é—®ï¼Œç”¨äºæ­£æ ·æœ¬å¯¹
        stand_simi_block_list = []
        for stand_group_item in cate_group_item[1].groupby(['Stand_text']):
            stand_text = stand_group_item[0][0]##æ–°ç‰ˆpandas-gropyby
            simi_text_list = stand_group_item[1]['Simi_text'].tolist()
            if stand_text not in simi_text_list:
                simi_text_list.append(stand_text)
            cate_simi_sample_nums_list.append(len(simi_text_list))
            stand_simi_block_list.append(simi_text_list)
        cate_2_simi_dict.update({cate_stand_tuple: stand_simi_block_list})
    cate_simi_new_dict = cate_2_simi_dict.copy()
    neg_stand0simi_pair_df = []
    pos_stand0simi_pair_df = []
    ## é‡å¤10æ¬¡;æˆ–è€…
    ## åŒæ—¶ä¿ç•™æ­£è´Ÿæ ·æœ¬å¯¹ã€‚
    loop_nums=pd.Series(cate_simi_sample_nums_list).describe()['50%']
    loop_nums=int(loop_nums)
    logger.info(f'Outer Simi Loop nums: {loop_nums}')
    for i in range(loop_nums):
        sub_neg_stand0simi_df, sub_pos_simi_df, stand_simi_new_dict = Inner_Exc_get_pair_df(cate_simi_new_dict,force_simi_comb=False)
        logger.info(f'sub_neg_stand0simi_df: {sub_neg_stand0simi_df.head()}')
        neg_stand0simi_pair_df.append(sub_neg_stand0simi_df)
        pos_stand0simi_pair_df.append(sub_pos_simi_df)
    sub_neg_stand0simi_df, sub_pos_simi_df, stand_simi_new_dict = Inner_Exc_get_pair_df(cate_simi_new_dict,force_simi_comb=True)
    neg_stand0simi_pair_df.append(sub_neg_stand0simi_df)
    pos_stand0simi_pair_df.append(sub_pos_simi_df)
    neg_stand_simi_pair_df = pd.concat(neg_stand0simi_pair_df, axis=0,sort=False)
    pos_stand_simi_pair_df = pd.concat(pos_stand0simi_pair_df, axis=0, sort=False)
    neg_stand_simi_pair_df['label'] = 0
    neg_stand_simi_pair_df['type'] = 'Neg_Inner_Diff_Simi'
    pos_stand_simi_pair_df['label'] = 1
    pos_stand_simi_pair_df['type'] = 'Pos_Inner_Simi'
    train_df = pd.concat([neg_stand_simi_pair_df, pos_stand_simi_pair_df], axis=0,sort=False).reset_index(drop=True)
    train_df.columns=['Stand_text', 'Simi_text', 'label','type']
    train_df = train_df.sample(frac=1).reset_index(drop=True)
    return train_df


#### M3-Inner Dev
## {ç§ç±»:[æ ‡å‡†é—®ä¸‹é¢çš„ç›¸ä¼¼é—®List,...,[]]
cate_2_simi_dict = {}
## å­˜å‚¨ç§ç±»ä¸‹ç›¸ä¼¼æ ·æœ¬æ•°é‡
cate_simi_sample_nums_list=[]
for cate_group_item in data_df.head(20).groupby(['Category']):
    cate_stand_tuple = cate_group_item[0][0]
    ## ä¸åŒæ ‡å‡†é—®å¯¹åº”çš„ç›¸ä¼¼é—®ï¼Œç”¨äºæ­£æ ·æœ¬å¯¹
    stand_simi_block_list=[]
    for stand_group_item in cate_group_item[1].groupby(['Stand_text']):
        stand_text=stand_group_item[0][0]
        print("stand_text: ",stand_text)
        simi_text_list=stand_group_item[1]['Simi_text'].tolist()
        if stand_text not in simi_text_list:
            simi_text_list.append(stand_text)
        cate_simi_sample_nums_list.append(len(simi_text_list))
        stand_simi_block_list.append(simi_text_list)
    cate_2_simi_dict.update({cate_stand_tuple:stand_simi_block_list})

pprint(cate_2_simi_dict)
print(pd.Series(cate_simi_sample_nums_list).describe())
print(pd.Series(cate_simi_sample_nums_list).describe()['25%'])


## å­˜å‚¨ç›¸åŒç§ç±»ä¸­çš„ä¸åŒæ ‡å‡†é—®-ç›¸ä¼¼é—®ï¼Œæ„æˆè´Ÿæ ·æœ¬
neg_diff_stand_2_simi_pair_list = []
## ç›¸ä¼¼é—®ä¹‹é—´ï¼Œæ„æˆæ­£æ ·æœ¬
pos_simi_text_pair_list = []
## ç§ç±»-[rest-æ ‡å‡†é—®_ç›¸ä¼¼é—®List]ï¼Œæ„æˆå­—å…¸
cate_rest_simi_dict = {}
force_simi_comb=False
for sub_category, sub_stand_simi_cont in cate_2_simi_dict.items():
    sub_cate_stand_simi_block_list_new = []
    ## å­˜å‚¨ç›¸åŒç§ç±»ä¸­çš„ä¸åŒæ ‡å‡†é—®-ç›¸ä¼¼é—®ï¼Œæ„æˆè´Ÿæ ·æœ¬
    sub_stand_2_simi_text_list = []
    for sub_stand_simi_list in sub_stand_simi_cont:
        rand_simi_text, rest_simi_text_list = data_split_list(sub_stand_simi_list)
        if rand_simi_text == None:
            continue
        sub_stand_2_simi_text_list.append(rand_simi_text)
        if force_simi_comb:
            ## å¼ºåˆ¶ç›¸ä¼¼æ ·æœ¬ç»„åˆæˆå¯¹
            rest_simi_text_pair_list = get_positive_pair(rest_simi_text_list)
            pos_simi_text_pair_list.extend(rest_simi_text_pair_list)
        elif len(rest_simi_text_list) // 2:
            ##éšæœºé€»è¾‘ï¼Œå½“ä¸ºå¥‡æ•°æ—¶ï¼Œç›¸ä¼¼æ ·æœ¬ç»„åˆæˆå¯¹
            rest_simi_text_pair_list = get_positive_pair(rest_simi_text_list)
            pos_simi_text_pair_list.extend(rest_simi_text_pair_list)
        ## ç›¸ä¼¼æ ·æœ¬ä¸ç»„åˆæˆå¯¹
        sub_cate_stand_simi_block_list_new.append(rest_simi_text_list)
    cate_rest_simi_dict.update({sub_category: sub_cate_stand_simi_block_list_new})
    ## ç›¸åŒç§ç±»çš„ä¸åŒæ ‡å‡†é—®çš„ç›¸ä¼¼é—®ï¼Œæ„é€ è´Ÿæ ·æœ¬å¯¹ï¼Œ
    neg_cate_pair_list = get_positive_pair(sub_stand_2_simi_text_list)
    neg_diff_stand_2_simi_pair_list.extend(neg_cate_pair_list)

neg_cate_simi_df=pd.DataFrame(neg_diff_stand_2_simi_pair_list)
pos_stand_simi_df=pd.DataFrame(pos_simi_text_pair_list)

logger.info(f'neg_cate_simi_df:\n{neg_cate_simi_df.head()}')
logger.info(f'pos_stand_simi_df:\n{pos_stand_simi_df.head()}')

#### Test

xx=pd.DataFrame({'Category':['A']*5+['H']*5,'Stand_text':['a1']*3+['a2']*2+['h1']*3+['h2']*2,
                 'Simi_text':list('bcdef')+list('ijkmn')})
logger.info(f'xx:\n{xx}')

train_df=Inner_Exc_sample_df_pre(xx)
logger.info(f'train_df:\n{train_df}')


def Inner_Exc_sample_focus_simi_neg_df(data_df,extra_file):
    """
    é‡ç‚¹åŸºäºç›¸åŒç§ç±»é—´-ç›¸ä¼¼è¯­æ–™æ„é€ è´Ÿæ ·æœ¬
    1. åŸºäºåŸºäºç›¸åŒç§ç±»çš„ç›¸ä¼¼é—®æ„é€ è´Ÿæ ·æœ¬-dfçš„base-method,ç”Ÿæˆdf
    2. é¢å¤–æ ·æœ¬[å‡è®¾ä¸ºæ ‡å‡†é—®]ä¸ç›¸ä¼¼æ ·æœ¬[åŸå§‹ç›¸ä¼¼]ï¼Œæ„é€ è´Ÿæ ·æœ¬é›†ã€‚
    :param data_df:DataFrame,åŸå§‹æ•°æ®ã€‚
    :param extra_file:é¢å¤–æ‰©å±•æ–‡ä»¶ï¼Œcsvæ ¼å¼
    return:train-df,åŸºäº1ï¼Œ2ä¸¤ç§æ€è·¯æ„é€ è´Ÿæ ·æœ¬è®­ç»ƒé›†ã€‚
    """
    train_df1 = Inner_Exc_sample_df_pre(data_df)
    train_df2 = Inner_Exc_sample_df_pre(data_df)
    train_data_pre = pd.concat([train_df1, train_df2], axis=0)
    label_dist = train_data_pre.label.value_counts()
    if label_dist.shape[0]>1:
        diff = label_dist[1] - label_dist[0]
        #æ­£è´Ÿæ ·æœ¬æ•°é‡å·®é¢
        diff = abs(diff)
        #print('diff: ',diff)
        data = data_df.copy()
        simi_text_series = data.loc[:, 'Simi_text'].sample(diff, replace=True)
    else:
        diff=int(label_dist/2)
        simi_text_series = train_data_pre.loc[:, 'Simi_text'].sample(diff)
    if extra_file:
        corpus_extra = pd.read_csv(extra_file, header=None)
        logger.info('Extra  data shape: {}'.format(corpus_extra.shape))
        if diff < corpus_extra.shape[0]:
            extra_neg_text = corpus_extra.sample(diff).loc[:, 0]
        else:
            extra_neg_text = corpus_extra.sample(diff, replace=True).loc[:, 0]
        extra_neg_df = pd.DataFrame({'Simi_text': simi_text_series.tolist(), 'Stand_text': extra_neg_text.tolist(), 'label': 0})
        extra_neg_df = extra_neg_df.loc[:, ['Stand_text', 'Simi_text', 'label']]
        extra_neg_df['type']='Neg_Extra_Simi_diff'
        ## è¿›ä¸€æ­¥å¢åŠ é¢å¤–æ ·æœ¬ä¸ç›¸ä¼¼é›†æ„é€ è´Ÿæ ·æœ¬å¯¹ï¼Œæ•°é‡ä¸ºåŸå§‹æ•°æ®é›†æ­£æ ·æœ¬ä¸€æ ·
        further_sample_nums=data_df.shape[0]
        logger.info(f'Extra further sample nums: {further_sample_nums}')
        simi_raw_text_series=data_df.loc[:, 'Simi_text'].sample(n=further_sample_nums)
        if further_sample_nums<corpus_extra.shape[0]:
            further_extra_neg_series=corpus_extra.sample(n=further_sample_nums).loc[:, 0]
        else:
            further_extra_neg_series = corpus_extra.sample(n=further_sample_nums,replace=True).loc[:, 0]
        extra_neg_df2 = pd.DataFrame(
            {'Simi_text': simi_raw_text_series.tolist(), 'Stand_text': further_extra_neg_series.tolist(), 'label': 0})
        extra_neg_df2 = extra_neg_df2.loc[:, ['Stand_text', 'Simi_text', 'label']]
        extra_neg_df2['type'] = 'Neg_Extra_Simi_rand'
        logger.info(f'extra_neg_df2 shape: {extra_neg_df2.shape}')
        all_data = pd.concat([train_data_pre,extra_neg_df,extra_neg_df2], axis=0,sort=False)
    else:
        all_data=train_data_pre
    all_data = all_data.sample(frac=1)
    return all_data

#### Test
xx=pd.DataFrame({'Category':['A']*5+['H']*5,'Stand_text':['a1']*3+['a2']*2+['h1']*3+['h2']*2,
                 'Simi_text':list('bcdef')+list('ijkmn')})
logger.info(f'xx:\n{xx}')

extra_file='data/simi_data/severe_simi_neg_extra_corpus.csv'
#extra_file=None
train_df=Inner_Exc_sample_focus_simi_neg_df(xx,extra_file)
logger.info(f'train_df shape: {train_df.shape}')
logger.info(f'train_df:\n{train_df.head()}')


def Inner_Exc_build_sample_df(data_df,extra_file):
    """
    1. æ­£æ ·æœ¬è‡ªæŠ½æ ·ï¼Œæ‰©å……æ­£æ ·æœ¬æ•°é‡,æ„é€ æ­£æ ·æœ¬å¯¹
    2. é¢å¤–æ‰©å±•æ ·æœ¬ä¸å…¶ä»–æ ‡å‡†é—®,æ„é€ è´Ÿæ ·æœ¬å¯¹
    3. å½“å‰æ ‡å‡†é—®ä¸å…¶ä»–æ ‡å‡†é—®ï¼ŒåŒ…å«ä¸å…¶ä»–æ‰€æœ‰é‡å¤ç¬›å¡å°”å’Œå»é‡æ ‡å‡†é—®ç¬›å¡å°”æˆå¯¹ï¼Œæ„æˆè´Ÿæ ·æœ¬å¯¹
    :param data_df: åªæœ‰æ­£æ ·æœ¬çš„åŸå§‹df,å¿…é¡»åŒ…å«Category,Stand_text,Simi_text,å…±3ä¸ªå­—æ®µ
    :param extra_file: é¢å¤–æ‰©å±•æ–‡ä»¶ï¼Œcsvæ ¼å¼
    return DataFrame: æŠ½æ ·ç»„æˆæœ€ç»ˆdf,å¿…é¡»åŒ…å«Category,Stand_text,Simi_text,labelå…±4ä¸ªå­—æ®µ
    """
    ## 1. æ­£æ ·æœ¬è‡ªæŠ½æ ·ï¼Œæ‰©å……æ­£æ ·æœ¬æ•°é‡
    data = data_df.copy()
    df_cols=['Stand_text','Simi_text','label','type']
    # è‡ªæŠ½æ ·æ‰©å……æ­£æ ·æœ¬æ•°é‡
    data_pos_sample_df1 = data.sample(frac=0.5)
    data_pos_sample_df2 = data.sample(frac=1.5, replace=True)
    data_pos_sample_df3 = data.loc[:, ['Category','Simi_text', 'Stand_text']].sample(frac=0.5)
    data_pos_sample_df3.columns=['Category','Stand_text','Simi_text']
    standard_query = data['Stand_text'].unique().tolist()
    # 2. é¢å¤–æ‰©å±•æ ·æœ¬ä¸å…¶ä»–æ ‡å‡†é—®ï¼Œæ„é€ è´Ÿæ ·æœ¬ã€‚
    if extra_file:
        data_extra = pd.read_csv(extra_file, header=None)
        data_extra = data_extra[0].unique().tolist()
        ## æ ‡å‡†é—®ä¸é¢å¤–æ ·æœ¬ç¬›å¡å°”ç§¯è·å¾—è´Ÿæ ·æœ¬å¯¹
        data_neg_extra_df4 = pd.DataFrame(list(itertools.product(standard_query, data_extra)))
        data_neg_extra_df4 = data_neg_extra_df4.sample(data.shape[0])
        data_neg_extra_df4.columns = ['Stand_text', 'Simi_text']
        data_neg_extra_df4['label']=0
        data_neg_extra_df4['type'] = 'Neg_Extra_Stand'
        data_neg_extra_df4=data_neg_extra_df4.loc[:,df_cols]
    else:
        data_neg_extra_df4=pd.DataFrame()
    #3.1 ç›¸åŒç§ç±»ä¸­å½“å‰æ ‡å‡†é—®ä¸å…¶ä»–æ ‡å‡†é—®ï¼Œä¸å…¶ä»–æ‰€æœ‰é‡å¤æ ‡å‡†é—®ç¬›å¡å°”
    sub_cate_unique_list=data['Category'].unique().tolist()
    sub_cate_inner_neg_stand_pair_list=[]
    # 3.2 ç›¸åŒç§ç±»ä¸­æ ‡å‡†é—®ä¹‹é—´å»é‡æ ‡å‡†é—®ç¬›å¡å°”æˆå¯¹
    sub_cate_inner_neg_stand_Descartes_pair_list = []
    for sub_cate in sub_cate_unique_list:
        sub_cate_df=data[data['Category']==sub_cate]
        sub_stand_unique_text = data['Stand_text'].unique().tolist()
        ## å»é‡æ ‡å‡†é—®ç¬›å¡å°”æˆå¯¹
        stand_unique_Descartes_pair_list = list(itertools.combinations(sub_stand_unique_text, 2))
        sub_cate_inner_neg_stand_Descartes_pair_list.extend(stand_unique_Descartes_pair_list)
        neg_stand_pair_list = []
        for sub_stand in sub_stand_unique_text:
            sub_other_stand_df = sub_cate_df[sub_cate_df['Stand_text'] != sub_stand]
            other_stand = sub_other_stand_df['Stand_text'].tolist()
            neg_stand_pair_list.extend(list(itertools.product([sub_stand], other_stand)))
        sub_cate_inner_neg_stand_pair_list.extend(neg_stand_pair_list)
    data_neg_stand_df5 = pd.DataFrame(sub_cate_inner_neg_stand_pair_list)
    data_neg_stand_df5 = data_neg_stand_df5.sample(data.shape[0])
    ## é‡æ ‡å‡†é—®ç¬›å¡å°”æˆå¯¹DF
    data_neg_stand_df6_pre = pd.DataFrame(sub_cate_inner_neg_stand_Descartes_pair_list)
    if data_neg_stand_df6_pre.shape[0] > int(data.shape[0] / 2):
        data_neg_stand_df6 = data_neg_stand_df6_pre.sample(int(len(data) / 2))
    else:
        data_neg_stand_df6 = data_neg_stand_df6_pre.sample(int(len(data) / 2), replace=True)
    #sample_pos_df = pd.concat([data_pos_sample_df1, data_pos_sample_df2, data_pos_sample_df3],axis=0,sort=False)
    sample_pos_df = pd.concat([data_pos_sample_df1, data_pos_sample_df2], axis=0, sort=False)
    sample_pos_df['label'] = 1
    sample_pos_df['type']='Self_sample'
    sample_pos_df=sample_pos_df.loc[:,df_cols]
    sample_neg_df=pd.concat([data_neg_stand_df5, data_neg_stand_df6],axis=0,sort=False)
    sample_neg_df.columns=['Stand_text','Simi_text']
    sample_neg_df['label']=0
    sample_neg_df['type'] = 'Neg_Stand_Interactive'
    sample_neg_df = sample_neg_df.loc[:, df_cols]
    #é‡ç‚¹åŸºäºç›¸ä¼¼é—®æ„é€ è´Ÿæ ·æœ¬å’Œä¸€å®šé‡çš„æ­£æ ·æœ¬
    train_focus_neg_df = Inner_Exc_sample_focus_simi_neg_df(data_df,extra_file)
    train_focus_neg_df=train_focus_neg_df.sample(len(data))
    logger.info(f'sample_pos_df shape:\n{sample_pos_df.shape}')
    logger.info(f'data_neg_extra_df4 shape:\n{data_neg_extra_df4.shape}')
    logger.info(f'sample_neg_df shape:\n{sample_neg_df.shape}')
    logger.info(f'train_focus_neg_df shape:\n{train_focus_neg_df.shape}')
    final_train_df = pd.concat([sample_pos_df,data_neg_extra_df4,sample_neg_df,train_focus_neg_df], axis=0, sort=False)
    final_train_df = final_train_df.sample(frac=1)
    final_train_df = final_train_df.dropna()
    final_train_df.loc[:, 'label'] = final_train_df.loc[:, 'label'].map(int)
    final_train_df = final_train_df.reset_index(drop=True)
    return final_train_df


#### Test
xx=pd.DataFrame({'Category':['A']*5+['H']*5,'Stand_text':['a1']*3+['a2']*2+['h1']*3+['h2']*2,
                 'Simi_text':list('bcdef')+list('ijkmn')})
logger.info(f'xx:\n{xx}')


extra_file='data/simi_data/severe_simi_neg_extra_corpus.csv'
#extra_file=None
train_df=Inner_Exc_build_sample_df(xx,extra_file)
logger.info(f'train_df shape:{train_df.shape}')
logger.info(f'train_df:\n{train_df.head()}')
logger.info(f'type distribution:\n{train_df.type.value_counts()}')
logger.info(f'label distribution:\n{train_df.label.value_counts()}')







