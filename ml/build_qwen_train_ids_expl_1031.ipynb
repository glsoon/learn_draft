{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 13:40:29,854 - INFO : Starting\n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\nQwen tokenizer注意事项：\\n1.特殊token用以给模型传递特殊信号，如到达文本末尾。 理论上，输入文本中不包含特殊token，它们仅在tokenization后由开发者手动加入。\\n特殊token的字面表达，如表示文本结束的<|endoftext|>，仅便于指代特殊token，不意味着它们在输入文本空间中。\\n2.在训练过程中，我们仅使用<|endoftext|>这一token作为sample/document之间的分隔符及padding位置占位符，\\n你可以将bos_id, eos_id, pad_id均指向tokenizer.eod_id。\\n3.Qwen模型结构类似于Llama.\\n4.预测从表明在【assistant+\\n】之后开始\\n5.训练label:\\n   system:不预测\\n   assistant:预测[im_start]+[assistant_text_token]+[im_end]+nl_tokens,不预测[assistant+\\n]\\n   注意：全程预测[im_start]+[im_end]+nl_tokens\\n'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime,timedelta,date\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModel, get_linear_schedule_with_warmup, \\\n",
    "    default_data_collator, TrainingArguments, Trainer\n",
    "from sentence_transformers import SentencesDataset, LoggingHandler, SentenceTransformer\n",
    "from sklearn.metrics.pairwise import paired_cosine_distances, paired_euclidean_distances, paired_manhattan_distances\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BertConfig, BertTokenizer,BertTokenizerFast\n",
    "from datasets import Dataset\n",
    "from typing import List,Union,Dict\n",
    "import tqdm\n",
    "import re\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "import os,time\n",
    "from pprint import pprint\n",
    "os.chdir('/home/stops/Work_space/NLP_work/Med_assit_chatglm')\n",
    "#os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "from db_config_taiyi import DB ## load data from pgsql\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO,format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger.info('Starting')\n",
    "\n",
    "\"\"\"\n",
    "Qwen tokenizer注意事项：\n",
    "1.特殊token用以给模型传递特殊信号，如到达文本末尾。 理论上，输入文本中不包含特殊token，它们仅在tokenization后由开发者手动加入。\n",
    "特殊token的字面表达，如表示文本结束的<|endoftext|>，仅便于指代特殊token，不意味着它们在输入文本空间中。\n",
    "2.在训练过程中，我们仅使用<|endoftext|>这一token作为sample/document之间的分隔符及padding位置占位符，\n",
    "你可以将bos_id, eos_id, pad_id均指向tokenizer.eod_id。\n",
    "3.Qwen模型结构类似于Llama.\n",
    "4.预测从表明在【assistant+\\n】之后开始\n",
    "5.训练label:\n",
    "   system:不预测\n",
    "   assistant:预测[im_start]+[assistant_text_token]+[im_end]+nl_tokens,不预测[assistant+\\n]\n",
    "   注意：全程预测[im_start]+[im_end]+nl_tokens\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 13:40:30,537 - INFO : load data from : data/BC2_train_Official_data_format_1010.json \n",
      "2023-12-18 13:40:30,538 - INFO : data nums : 203 \n",
      "2023-12-18 13:40:30,538 - INFO : examples : [{'id': 0, 'conversations': [{'from': 'human', 'value': '模拟问诊,你的角色是小太医医生，有丰富的医疗问诊知识和经验，如果患者有买药意图，医生回复应该追加“稍后会有医生为您开药。[开药]'}, {'from': 'gpt', 'value': '您好，我是小太医，有什么可以帮到您的？'}, {'from': 'human', 'value': '我发烧了'}, {'from': 'gpt', 'value': '您好，请告诉我一下您发热有多久了？体温最高是多少呢，目前的体温是多少？'}, {'from': 'human', 'value': '得有2天了。目前37.8度'}, {'from': 'gpt', 'value': '这段时间内除了发热之外还有其他伴随症状吗？例如鼻塞流涕、咽痛、咳嗽、咳痰等症状，\\n有没有呕吐、腹泻、尿频、尿急、尿痛呢？'}, {'from': 'human', 'value': '没有，就嘴巴干'}, {'from': 'gpt', 'value': '了解了，出现这些情况之前是否有受凉、淋雨、不洁饮食等情况呢？'}, {'from': 'human', 'value': '没哦'}, {'from': 'gpt', 'value': '好的，之前是否有出现过类似问题，有无其他什么基础疾病呢？比如风湿、肿瘤、甲亢、结缔组织病等，有没有出血、大手术、烧伤等情况呢？'}, {'from': 'human', 'value': '没'}, {'from': 'gpt', 'value': '您的发热可能是由于感染引起的。根据您的症状，暂时没有其他明显的伴随症状，所以可能是一种轻微的感冒或者病毒性感染。建议您注意以下几点：\\n1. 饮食方面，多喝水，保持充足的水分摄入，可以喝一些温开水、绿茶、红枣汤等，有助于缓解嘴巴干的感觉。\\n2. 避免辛辣、油腻、刺激性食物，以免加重症状。\\n3. 多休息，保持充足的睡眠，有助于提高免疫力，促进身体康复。\\n4. 可以适量服用退烧药物，如布洛芬或对乙酰氨基酚，但请务必按照药品说明或医生指导的剂量使用。\\n如果您的体温持续升高或出现其他症状，请及时就医，以便进行进一步检查和治疗。祝您早日康复！本次服务对您是否有帮助，请进行评价，谢谢。'}]}, {'id': 1, 'conversations': [{'from': 'human', 'value': '模拟问诊,你的角色是小太医医生，有丰富的医疗问诊知识和经验，如果患者有买药意图，医生回复应该追加“稍后会有医生为您开药。[开药]'}, {'from': 'gpt', 'value': '您好，我是小太医，有什么可以帮到您的？'}, {'from': 'human', 'value': '问一下，我发烧了'}, {'from': 'gpt', 'value': '请告诉我您具体发热有多久了？\\n体温最高是多少呢，目前的体温是多少？'}, {'from': 'human', 'value': '发烧了3个小时，38.2度'}, {'from': 'gpt', 'value': '这段时间内除了发烧还有其他不舒服的地方吗？例如鼻塞流涕、咽痛、咳嗽、咳痰等症状呢？\\n有没有呕吐、腹泻、尿频、尿急、尿痛呢？'}, {'from': 'human', 'value': '有，头痛，然后还有鼻子塞住 流鼻涕，嗓子也疼'}, {'from': 'gpt', 'value': '出现这些情况之前是否有受凉、淋雨、不洁饮食等情况呢？'}, {'from': 'human', 'value': '那到没有的'}, {'from': 'gpt', 'value': '关于你的诊疗经过，请告诉我这次有没有去线下医院就诊过或者自行服用药物治疗呢？有做过什么检查吗？比如血常规、C反应蛋白、胸片或者胸部CT等，如果有可以把报告单上传给我看一下。\\n针对目前的情况，您有没有服用相关的药物呢？比如布洛芬缓释胶囊、散列通等，服药治疗后效果怎么样？'}, {'from': 'human', 'value': '吃了退烧药，是布洛芬'}, {'from': 'gpt', 'value': '您好，根据您的症状描述，您可能患有感冒或流感。建议您进行以下饮食和治疗措施：\\n饮食建议：\\n1. 多喝温水，保持水分摄入，有助于预防脱水。\\n2. 食用易消化、营养丰富的食物，如清汤、蔬菜、水果和煮熟的鸡肉等。\\n3. 避免食用刺激性食物，如辛辣食物、油炸食物和咖啡因饮料。\\n治疗建议：\\n1. 休息充足，避免过度劳累，有助于身体康复。\\n2. 保持室内空气流通，保持适宜的温度和湿度。\\n3. 使用温水漱口缓解咽痛症状。\\n4. 鼻塞可使用盐水清洗鼻腔，或者使用鼻通剂缓解症状。\\n5. 如果体温持续升高，可以考虑继续服用布洛芬缓释胶囊或其他退烧药物，但请按照医生或药剂师的建议使用，并注意药物的剂量和频率。\\n请注意，以上建议仅供参考。如果症状加重或持续超过3天，建议您及时就医并咨询医生的意见。祝您早日康复！本次服务对您是否有帮助，请进行评价，谢谢。'}]}, {'id': 2, 'conversations': [{'from': 'human', 'value': '模拟问诊,你的角色是小太医医生，有丰富的医疗问诊知识和经验，如果患者有买药意图，医生回复应该追加“稍后会有医生为您开药。[开药]'}, {'from': 'gpt', 'value': '您好，我是小太医，有什么可以帮到您的？'}, {'from': 'human', 'value': '问一下，我肚子痛'}, {'from': 'gpt', 'value': '您好，腹痛从最开始出现到现在有多久了？腹痛每次持续多久？大概多久出现一次？'}, {'from': 'human', 'value': '有11天了,肚子反复痛。'}, {'from': 'gpt', 'value': '这段时间里，您感觉腹痛在哪个位置？'}, {'from': 'human', 'value': '不知道，没感觉出来'}, {'from': 'gpt', 'value': '在疼痛的时候，可以描述一下有什么特殊的感觉吗？是一阵一阵痛呢还是一直痛？是剧烈痛还是隐隐作痛？'}, {'from': 'human', 'value': '肚子胀的很'}, {'from': 'gpt', 'value': '有其他伴随的不适感吗？比如恶心、呕吐、腹胀、腹泻、发热、便血、肛门停止排气排便'}, {'from': 'human', 'value': '没有了'}, {'from': 'gpt', 'value': '痛之前有没有吃什么特别的东西？比如所辛辣刺激的食物、冷饮、烧烤等。腹部有没有受到撞击？'}, {'from': 'human', 'value': '没有'}, {'from': 'gpt', 'value': '好的，再此期间有没有去医院检查过？比如说做过胃镜、肠镜、肛门镜、直肠指检、腹腔彩超、腹腔CT等。\\n您有没有服用相关的药物呢？比如奥美拉唑、6542、布洛芬等。服药治疗后效果怎么样？'}, {'from': 'human', 'value': '吃了点药，有肠炎宁、枯草杆菌1天。昨天停了药之后又开始反复了'}, {'from': 'gpt', 'value': '感谢提供这些信息。除了这次的症状之外，请问是否有高血压、糖尿病、高脂血症、心脏病、脑血管病、长期腹泻、便秘、前列腺增生？以前有无类似发作？ 以前有没有胆道、胃肠道方面的疾病？'}, {'from': 'human', 'value': '有点，之前有高血压，高血脂，脑血管病，可疑性冠心病，还有肠炎'}, {'from': 'gpt', 'value': '根据您的描述，您的症状可能与消化系统有关。由于您之前有高血压、高血脂、脑血管病等病史，建议您进行一些进一步的检查，如腹部超声、胃镜等，以排除其他潜在的疾病。此外，您之前服用的药物可能对您的症状产生影响，建议您停止自行服用药物，等待进一步的检查结果。在等待检查期间，为您提供以下饮食和治疗建议：\\n1. 饮食方面：\\n   - 避免食用辛辣、刺激性食物，如辣椒、花椒、生姜等。\\n   - 尽量避免油腻、高脂肪食物，如油炸食品、猪肉等。\\n   - 建议多食用富含纤维的食物，如蔬菜、水果、全谷类食物。\\n   - 注意饮食规律，避免过饱或过饿的状态。\\n   - 建议分餐进食，少量多餐，避免暴饮暴食。\\n2. 治疗方面：\\n   - 建议您遵医嘱停止使用药物，直至进一步的检查结果出来。\\n   - 您可以尝试使用热水袋敷在腹部，以缓解胀痛感。\\n   - 如果疼痛加剧或伴随其他不适症状，如呕吐、发热等，及时就医。\\n请注意，以上建议仅供参考，具体的治疗方案还需要根据您的检查结果和医生的诊断来确定。建议您尽快就医，并告知医生您的病史和症状，以便得到准确的诊断和治疗。本次服务对您是否有帮助，请进行评价，谢谢。'}]}, {'id': 3, 'conversations': [{'from': 'human', 'value': '模拟问诊,你的角色是小太医医生，有丰富的医疗问诊知识和经验，如果患者有买药意图，医生回复应该追加“稍后会有医生为您开药。[开药]'}, {'from': 'gpt', 'value': '您好，我是小太医，有什么可以帮到您的？'}, {'from': 'human', 'value': '医生，我腹痛'}, {'from': 'gpt', 'value': '您好，你能描述一下这个腹痛的感觉吗？腹痛每次持续多久？大概多久出现一次？是一阵一阵痛呢还是一直痛？是剧烈痛还是隐隐作痛？腹痛在哪个位置？'}, {'from': 'human', 'value': '应该是胰腺部位，左边是不是疼，还涨。肋疼痛'}, {'from': 'gpt', 'value': '腹痛每次持续多久？大概多久出现一次？'}, {'from': 'human', 'value': '有1个月左右了'}, {'from': 'gpt', 'value': '好的，那么在过去的一个月里，痛之前有没有吃什么特别的东西？比如所辛辣刺激的食物、冷饮、烧烤等。腹部有没有受到撞击？'}, {'from': 'human', 'value': '没有吃'}, {'from': 'gpt', 'value': '在这一个月内，你是否尝试过其他的治疗方法来缓解疼痛？有没有去医院检查过？比如说做过胃镜、肠镜、肛门镜、直肠指检、腹腔彩超、腹腔CT等。\\n针对目前的情况，您有没有服用相关的药物呢？比如奥美拉唑、6542、布洛芬等。服药治疗后效果怎么样？'}, {'from': 'human', 'value': '也没有'}, {'from': 'gpt', 'value': '最后一个问题，除了这次腹痛之外，你还曾经有过类似的情况吗？请问是否有高血压、糖尿病、高脂血症、心脏病、脑血管病、长期腹泻、便秘、前列腺增生？以前有无类似发作？ 以前有没有胆道、胃肠道方面的疾病？'}, {'from': 'human', 'value': '无'}, {'from': 'gpt', 'value': '根据你的描述，你可能患有胰腺炎。胰腺炎是胰腺发炎引起的腹痛。建议你进行以下饮食和治疗调整：\\n1. 饮食建议：\\n- 避免食用辛辣、油腻、煎炸食品，这些食物可能刺激胰腺，加重症状。\\n- 尽量少吃高脂肪食物，如油炸食品、动物脂肪和全脂乳制品。\\n- 建议选择低脂肪、高纤维的食物，如蔬菜、水果、全谷物和瘦肉。\\n- 多喝水以保持身体水分平衡。\\n2. 治疗建议：\\n- 由于你没有尝试过其他治疗方法，建议你去医院进一步检查，例如做胃镜、肠镜等，以确定病因并制定更具针对性的治疗方案。\\n- 在等待医院检查的过程中，可以尝试服用非处方的抗酸药物来缓解疼痛，如奥美拉唑。\\n- 避免过度用药，如果疼痛严重或持续加剧，及时就医。\\n\\n请记住，以上建议仅供参考，最好还是咨询医生进行进一步诊断和治疗。本次服务对您是否有帮助，请进行评价，谢谢。'}]}, {'id': 4, 'conversations': [{'from': 'human', 'value': '模拟问诊,你的角色是小太医医生，有丰富的医疗问诊知识和经验，如果患者有买药意图，医生回复应该追加“稍后会有医生为您开药。[开药]'}, {'from': 'gpt', 'value': '您好，我是小太医，有什么可以帮到您的？'}, {'from': 'human', 'value': '问一下，我的肚子痛'}, {'from': 'gpt', 'value': '这段时间您疼痛的程度如何？是一阵一阵痛呢还是一直痛？是剧烈痛还是隐隐作痛？\\n腹痛在哪个位置？'}, {'from': 'human', 'value': '不知道，没有规律，还有打喷嚏呢'}, {'from': 'gpt', 'value': '好的，这些信息很有帮助。\\n请问这个腹痛每次持续多久？大概多久出现一次？'}, {'from': 'human', 'value': '有2天啦'}, {'from': 'gpt', 'value': '痛之前有没有吃什么特别的东西？比如所辛辣刺激的食物、冷饮、烧烤等。腹部有没有受到撞击？'}, {'from': 'human', 'value': '好像是吃的蛋挞有问题'}, {'from': 'gpt', 'value': '我明白了。除了这次的腹痛以外，有没有去医院检查过？比如说做过胃镜、肠镜、肛门镜、直肠指检、腹腔彩超、腹腔CT等。是否有高血压、糖尿病、胆囊炎、胃溃疡等疾病史？'}, {'from': 'human', 'value': '没有，我以前都挺健康的'}, {'from': 'gpt', 'value': '好的，根据您的症状描述，可能是蛋挞引起的胃部不适。建议您暂时避免吃过于油腻、刺激性的食物，尽量选择清淡易消化的食物，如米粥、面条、煮蔬菜等。同时，多喝水，保持肠道通畅。如果疼痛加重或持续超过3天，建议您及时就医进行进一步检查。本次服务对您是否有帮助，请进行评价，谢谢。'}]}] \n"
     ]
    }
   ],
   "source": [
    "data_file=\"data/BC2_train_Official_data_format_1010.json\"\n",
    "\n",
    "with open(data_file,\"r\") as f:\n",
    "    conversation_data_list=json.load(f)\n",
    "\n",
    "logger.info(f\"load data from : {data_file} \")\n",
    "logger.info(f\"data nums : {len(conversation_data_list)} \")\n",
    "logger.info(f\"examples : {conversation_data_list[:5]} \")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 14:03:01,846 - INFO : load model : /home/stops/Work_space/NLP_models/Qwen-14B-Chat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151643\n",
      "Pad id:  None\n",
      "151643\n",
      "['<|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "model_path=\"/home/stops/Work_space/NLP_models/Qwen-14B-Chat\"\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_path,trust_remote_code=True)\n",
    "logger.info(f\"load model : {model_path}\")\n",
    "\n",
    "## 注意设置pad\n",
    "print(\"Pad id: \",print(tokenizer.eod_id))\n",
    "tokenizer.pad_token_id = tokenizer.eod_id\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.convert_ids_to_tokens([tokenizer.pad_token_id]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im_start:  151644\n",
      "im_end:  151645\n",
      "nl_tokens:  [198]\n",
      "_system:  [8948, 198]\n",
      "_user:  [872, 198]\n",
      "_assistant:  [77091, 198]\n"
     ]
    }
   ],
   "source": [
    "im_start = tokenizer.im_start_id\n",
    "im_end = tokenizer.im_end_id\n",
    "nl_tokens = tokenizer('\\n').input_ids\n",
    "_system = tokenizer('system').input_ids + nl_tokens\n",
    "_user = tokenizer('user').input_ids + nl_tokens\n",
    "_assistant = tokenizer('assistant').input_ids + nl_tokens\n",
    "\n",
    "print(\"im_start: \",im_start)\n",
    "print(\"im_end: \",im_end)\n",
    "print(\"nl_tokens: \",nl_tokens)\n",
    "print(\"_system\\n: \",_system)\n",
    "print(\"_user\\n: \",_user)\n",
    "print(\"_assistant\\n: \",_assistant)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[151644]\n",
      "[151645]\n",
      "[151643]\n",
      "151643\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer('<|im_start|>').input_ids)## tokenizer.im_start_id,\n",
    "print(tokenizer('<|im_end|>').input_ids) ## tokenizer.im_end_id\n",
    "print(tokenizer(\"<|endoftext|>\").input_ids)\n",
    "print(tokenizer.pad_token_id)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[151644, 872]\n",
      "[151644, 77091]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer('<|im_start|>user').input_ids)\n",
    "print(tokenizer('<|im_start|>assistant').input_ids)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.由于都是手动添加：im_start，im_end，nl_tokens，故这三个special-token不预测，但是要显性出现。\n",
    "2.全部对话从[assistant+\\n]开始，只预测[assistant_text_token],但不预测[assistant+\\n]\n",
    "\"\"\"\n",
    "\n",
    "IGNORE_TOKEN_ID=-100\n",
    "\n",
    "def preprocess(sources,tokenizer,max_len,system_message= \"You are a helpful assistant.\") -> Dict:\n",
    "    roles = {\"user\": \"<|im_start|>user\", \"assistant\": \"<|im_start|>assistant\"}\n",
    "\n",
    "    im_start = tokenizer.im_start_id\n",
    "    im_end = tokenizer.im_end_id\n",
    "    nl_tokens = tokenizer('\\n').input_ids\n",
    "    _system = tokenizer('system').input_ids + nl_tokens\n",
    "    _user = tokenizer('user').input_ids + nl_tokens\n",
    "    _assistant = tokenizer('assistant').input_ids + nl_tokens\n",
    "\n",
    "    # Apply prompt templates\n",
    "    input_ids, targets = [], []\n",
    "    for i, source in enumerate(sources):\n",
    "        ## 要求首个角色必须为User\n",
    "        if roles[source[0][\"from\"]] != roles[\"user\"]:\n",
    "            source = source[1:]\n",
    "\n",
    "        input_id, target = [], []\n",
    "        system = [im_start] + _system + tokenizer(system_message).input_ids + [im_end] + nl_tokens\n",
    "        input_id += system\n",
    "        #由于都是手动添加：im_start，im_end，nl_tokens，故这三个special-token不预测。\n",
    "        target += [im_start] + [IGNORE_TOKEN_ID] * (len(system)-3) + [im_end] + nl_tokens\n",
    "        print(\"system len: \",len(system),\"target len: \",len(target))\n",
    "        assert len(input_id) == len(target)\n",
    "        for j, sentence in enumerate(source):\n",
    "            role = roles[sentence[\"from\"]]\n",
    "            ## 统一处理文本，核心是信息文本，其他token[im_start，im_end，nl_tokens]手动添加，所以模型不应该预测。\n",
    "            _input_id = tokenizer(role).input_ids + nl_tokens + tokenizer(sentence[\"value\"]).input_ids + [im_end] + nl_tokens\n",
    "            input_id += _input_id\n",
    "\n",
    "            ## 按不同角色处理不同信息文本，用户角色文本不预测，重点预测助手的文本内容。\n",
    "            if role == '<|im_start|>user':\n",
    "                #由于都是手动添加：im_start，im_end，nl_tokens，故这三个special-token不预测。\n",
    "                _target = [im_start] + [IGNORE_TOKEN_ID] * (len(_input_id)-3) + [im_end] + nl_tokens\n",
    "            elif role == '<|im_start|>assistant':\n",
    "                #由于都是手动添加：im_start，im_end，nl_tokens，故这三个special-token不预测。\n",
    "                #只预测[assistant_text_token],但不预测[assistant+\\n]\n",
    "                _target = [im_start] + [IGNORE_TOKEN_ID] * len(tokenizer(role).input_ids) + \\\n",
    "                    _input_id[len(tokenizer(role).input_ids)+1:-2] + [im_end] + nl_tokens\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            target += _target\n",
    "            print(\"_input_id len: \",len(system),\"_target len: \",len(target))\n",
    "        assert len(input_id) == len(target)\n",
    "        input_id += [tokenizer.pad_token_id] * (max_len - len(input_id))\n",
    "        target += [IGNORE_TOKEN_ID] * (max_len - len(target))\n",
    "        input_ids.append(input_id[:max_len])\n",
    "        targets.append(target[:max_len])\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.int)\n",
    "    targets = torch.tensor(targets, dtype=torch.int)\n",
    "\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=targets,\n",
    "        attention_mask=input_ids.ne(tokenizer.pad_token_id))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'from': 'user', 'value': '在吗'}, {'from': 'assistant', 'value': '在的'}, {'from': 'user', 'value': '头痛'}, {'from': 'assistant', 'value': '几天了'}]]\n"
     ]
    }
   ],
   "source": [
    "test_data=[[{'from': 'user', 'value': '在吗'},\n",
    "            {'from': 'assistant', 'value': '在的'},\n",
    "            {'from': 'user', 'value': '头痛'},\n",
    "            {'from': 'assistant', 'value': '几天了'},]]\n",
    "print(test_data)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system len:  11 target len:  11\n",
      "{'attention_mask': tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True, False, False]]),\n",
      " 'input_ids': tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,\n",
      "         151645,    198, 151644,    872,    198,  18493, 101037, 151645,    198,\n",
      "         151644,  77091,    198,  18493,   9370, 151645,    198, 151644,    872,\n",
      "            198, 109180, 151645,    198, 151644,  77091,    198, 101437,  34187,\n",
      "         151645,    198, 151643, 151643]], dtype=torch.int32),\n",
      " 'labels': tensor([[151644,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "         151645,    198, 151644,   -100,   -100,   -100,   -100, 151645,    198,\n",
      "         151644,   -100,   -100,  18493,   9370, 151645,    198, 151644,   -100,\n",
      "           -100,   -100, 151645,    198, 151644,   -100,   -100, 101437,  34187,\n",
      "         151645,    198,   -100,   -100]], dtype=torch.int32)}\n"
     ]
    }
   ],
   "source": [
    "test_result_ids=preprocess(test_data,tokenizer,max_len=40)\n",
    "pprint(test_result_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nums:  40\n",
      "151644 ====> 151644\n",
      "8948 ====> -100\n",
      "198 ====> -100\n",
      "2610 ====> -100\n",
      "525 ====> -100\n",
      "264 ====> -100\n",
      "10950 ====> -100\n",
      "17847 ====> -100\n",
      "13 ====> -100\n",
      "151645 ====> 151645\n",
      "198 ====> 198\n",
      "151644 ====> 151644\n",
      "872 ====> -100\n",
      "198 ====> -100\n",
      "18493 ====> -100\n",
      "101037 ====> -100\n",
      "151645 ====> 151645\n",
      "198 ====> 198\n",
      "151644 ====> 151644\n",
      "77091 ====> -100\n",
      "198 ====> -100\n",
      "18493 ====> 18493\n",
      "9370 ====> 9370\n",
      "151645 ====> 151645\n",
      "198 ====> 198\n",
      "151644 ====> 151644\n",
      "872 ====> -100\n",
      "198 ====> -100\n",
      "109180 ====> -100\n",
      "151645 ====> 151645\n",
      "198 ====> 198\n",
      "151644 ====> 151644\n",
      "77091 ====> -100\n",
      "198 ====> -100\n",
      "101437 ====> 101437\n",
      "34187 ====> 34187\n",
      "151645 ====> 151645\n",
      "198 ====> 198\n",
      "151643 ====> -100\n",
      "151643 ====> -100\n"
     ]
    }
   ],
   "source": [
    "print(\"nums: \",len(test_result_ids[\"input_ids\"].numpy()[0]))\n",
    "for x,y in zip(test_result_ids[\"input_ids\"].numpy()[0],test_result_ids[\"labels\"].numpy()[0]):\n",
    "    print(x,\"====>\",y)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|> : 151644 ----> -100   :       \n",
      "system :   8948 ----> -100   :       \n",
      "\n",
      "      :    198 ----> -100   :       \n",
      "You    :   2610 ----> -100   :       \n",
      " are   :    525 ----> -100   :       \n",
      " a     :    264 ----> -100   :       \n",
      " helpful :  10950 ----> -100   :       \n",
      " assistant :  17847 ----> -100   :       \n",
      ".      :     13 ----> 151645 : <|im_end|>\n",
      "<|im_end|> : 151645 ----> 198    :       \n",
      "\n",
      "      :    198 ----> 151644 : <|im_start|>\n",
      "<|im_start|> : 151644 ----> -100   :       \n",
      "user   :    872 ----> -100   :       \n",
      "\n",
      "      :    198 ----> -100   :       \n",
      "在      :  18493 ----> -100   :       \n",
      "吗      : 101037 ----> 151645 : <|im_end|>\n",
      "<|im_end|> : 151645 ----> 198    :       \n",
      "\n",
      "      :    198 ----> 151644 : <|im_start|>\n",
      "<|im_start|> : 151644 ----> -100   :       \n",
      "assistant :  77091 ----> -100   :       \n",
      "\n",
      "      :    198 ----> 18493  :      在\n",
      "在      :  18493 ----> 9370   :      的\n",
      "的      :   9370 ----> 151645 : <|im_end|>\n",
      "<|im_end|> : 151645 ----> 198    :       \n",
      "\n",
      "      :    198 ----> 151644 : <|im_start|>\n",
      "<|im_start|> : 151644 ----> -100   :       \n",
      "user   :    872 ----> -100   :       \n",
      "\n",
      "      :    198 ----> -100   :       \n",
      "头痛     : 109180 ----> 151645 : <|im_end|>\n",
      "<|im_end|> : 151645 ----> 198    :       \n",
      "\n",
      "      :    198 ----> 151644 : <|im_start|>\n",
      "<|im_start|> : 151644 ----> -100   :       \n",
      "assistant :  77091 ----> -100   :       \n",
      "\n",
      "      :    198 ----> 101437 :     几天\n",
      "几天     : 101437 ----> 34187  :      了\n",
      "了      :  34187 ----> 151645 : <|im_end|>\n",
      "<|im_end|> : 151645 ----> 198    :       \n",
      "\n",
      "      :    198 ----> -100   :       \n",
      "<|endoftext|> : 151643 ----> -100   :       \n"
     ]
    }
   ],
   "source": [
    "for x,y in zip(test_result_ids[\"input_ids\"].numpy()[0][:-1],test_result_ids[\"labels\"].numpy()[0][1:]):\n",
    "    x_token=tokenizer.convert_ids_to_tokens([x])[0]\n",
    "    y_token=tokenizer.convert_ids_to_tokens([y])[0] if y not in [-100,198] else \"\"\n",
    "    #print(type(x_token),x_token,type(y_token),y_token)\n",
    "    if isinstance(x_token,bytes):\n",
    "        x_token=x_token.decode(\"utf8\")\n",
    "    if isinstance(y_token,bytes):\n",
    "        y_token=y_token.decode(\"utf8\")\n",
    "    print(r\"{:<6s} : {:>6d} ----> {:<6d} : {:>6s}\".format(x_token,x,y,y_token))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#########################\n",
    "## 推理的表达式\n",
    "#########################\n",
    "from typing import Tuple\n",
    "\n",
    "## history:history.append((query, response))\n",
    "\n",
    "def make_context(tokenizer,query: str,history: List[Tuple[str, str]] = None,system: str = \"\",\n",
    "    max_window_size: int = 6144,chat_format: str = \"chatml\",):\n",
    "    if history is None:\n",
    "        history = []\n",
    "\n",
    "    if chat_format == \"chatml\":\n",
    "        im_start, im_end = \"<|im_start|>\", \"<|im_end|>\"\n",
    "        im_start_tokens = [tokenizer.im_start_id]\n",
    "        im_end_tokens = [tokenizer.im_end_id]\n",
    "        nl_tokens = tokenizer.encode(\"\\n\")\n",
    "\n",
    "        def _tokenize_str(role, content):\n",
    "            return f\"{role}\\n{content}\", tokenizer.encode(\n",
    "                role, allowed_special=set()\n",
    "            ) + nl_tokens + tokenizer.encode(content, allowed_special=set())\n",
    "\n",
    "        system_text, system_tokens_part = _tokenize_str(\"system\", system)\n",
    "        system_tokens = im_start_tokens + system_tokens_part + im_end_tokens\n",
    "\n",
    "        raw_text = \"\"\n",
    "        context_tokens = []\n",
    "\n",
    "        for turn_query, turn_response in reversed(history):\n",
    "            query_text, query_tokens_part = _tokenize_str(\"user\", turn_query)\n",
    "            query_tokens = im_start_tokens + query_tokens_part + im_end_tokens\n",
    "            response_text, response_tokens_part = _tokenize_str(\"assistant\", turn_response)\n",
    "            response_tokens = im_start_tokens + response_tokens_part + im_end_tokens\n",
    "\n",
    "            next_context_tokens = nl_tokens + query_tokens + nl_tokens + response_tokens\n",
    "            prev_chat = (f\"\\n{im_start}{query_text}{im_end}\\n{im_start}{response_text}{im_end}\")\n",
    "\n",
    "            current_context_size = (len(system_tokens) + len(next_context_tokens) + len(context_tokens))\n",
    "            if current_context_size < max_window_size:\n",
    "                context_tokens = next_context_tokens + context_tokens\n",
    "                raw_text = prev_chat + raw_text\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        context_tokens = system_tokens + context_tokens\n",
    "        raw_text = f\"{im_start}{system_text}{im_end}\" + raw_text\n",
    "        context_tokens += (\n",
    "            nl_tokens\n",
    "            + im_start_tokens\n",
    "            + _tokenize_str(\"user\", query)[1]\n",
    "            + im_end_tokens\n",
    "            + nl_tokens\n",
    "            + im_start_tokens\n",
    "            + tokenizer.encode(\"assistant\")\n",
    "            + nl_tokens\n",
    "        )## 表明在assistant+\\n之后开始预测\n",
    "        raw_text += f\"\\n{im_start}user\\n{query}{im_end}\\n{im_start}assistant\\n\"\n",
    "\n",
    "    elif chat_format == \"raw\":\n",
    "        raw_text = query\n",
    "        context_tokens = tokenizer.encode(raw_text)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Unknown chat format {chat_format!r}\")\n",
    "\n",
    "    return raw_text, context_tokens\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"注意换行符号\n",
    "system\n",
    "system_text\n",
    "user\n",
    "user_text\n",
    "assistant\n",
    "assistant_text\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "system_tokens   : im_start_tokens + system_token      + nl_tokens + system_text_token      +  im_end_tokens\n",
    "query_tokens    : im_start_tokens + user_token        + nl_tokens + user_text_token        +  im_end_tokens\n",
    "response_tokens : im_start_tokens + assistant_token   + nl_tokens + assistant_text_token   +  im_end_tokens\n",
    "\n",
    "##loop:\n",
    "next_context_tokens = nl_tokens + query_tokens + nl_tokens + response_tokens\n",
    "\n",
    "context_tokens =system_tokens+next_context_tokens\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "llm_env",
   "language": "python",
   "display_name": "LLM_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
